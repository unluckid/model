{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jX7ukQPOAxrM"
      },
      "source": [
        "<table align=\"left\">\n",
        "  <td>\n",
        "    <a href=\"https://colab.research.google.com/github/unluckid/model/blob/main/model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sFcMpxP2AxrN",
        "tags": []
      },
      "source": [
        "# 설정"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "xNKQVwanAxrN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        },
        "outputId": "ba38ceb9-5704-4d9b-e535-8066fac8a864"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1oPazCje0mkB101pzkY1ySCW6g750-KRK\n",
            "From (redirected): https://drive.google.com/uc?id=1oPazCje0mkB101pzkY1ySCW6g750-KRK&confirm=t&uuid=06a98586-eff3-4cb7-ba44-472da8ed0002\n",
            "To: /content/vgg16.keras\n",
            "100%|██████████| 301M/301M [00:03<00:00, 80.6MB/s]\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1jkAAF7VwsXaBh2DgTkP5w9icS1FfutOR\n",
            "From (redirected): https://drive.google.com/uc?id=1jkAAF7VwsXaBh2DgTkP5w9icS1FfutOR&confirm=t&uuid=8893eefa-5502-44b1-82a1-f255489fae4c\n",
            "To: /content/inceptionV3.keras\n",
            "100%|██████████| 403M/403M [00:03<00:00, 117MB/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/inceptionV3.keras'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "import gdown\n",
        "import numpy as np\n",
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "import shutil\n",
        "file_id = '1jkAAF7VwsXaBh2DgTkP5w9icS1FfutOR'\n",
        "file_id1 = '1oPazCje0mkB101pzkY1ySCW6g750-KRK'\n",
        "download_url = f'https://drive.google.com/uc?id={file_id}'\n",
        "download_url1 = f'https://drive.google.com/uc?id={file_id1}'\n",
        "\n",
        "# 다운로드 시작\n",
        "\n",
        "gdown.download(download_url1, '/content/vgg16.keras', quiet=False)\n",
        "gdown.download(download_url, '/content/inceptionV3.keras', quiet=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "0Ddo9OPsAxrO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f13a36c7-1ce6-4463-d651-465803a3b88a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "압축이 /content/에 해제되었습니다.\n"
          ]
        }
      ],
      "source": [
        "import zipfile\n",
        "\n",
        "\n",
        "zip_file_path = '/content/dataset.zip'\n",
        "extract_path = '/content/'\n",
        "\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n",
        "\n",
        "print(f\"압축이 {extract_path}에 해제되었습니다.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.makedirs(\"train/\", exist_ok=True)\n",
        "os.makedirs(\"val/\", exist_ok=True)"
      ],
      "metadata": {
        "id": "cvsDrFm3i-MP"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "JN8_1_vCAxrP"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "class Image_propressing:\n",
        "    def __init__(self):\n",
        "        #  폴더 경로\n",
        "        self.video_path = \"video/\"  # 비디오 폴더\n",
        "        self.nature_img_folder_path = \"nature/\"  # 전처리 이전 폴더\n",
        "        self.model1_img_folder_path = \"model1/\"  # 기본 모델 폴더\n",
        "        self.model2_img_folder_path = \"model2/\"  # 캐니 모델 폴더\n",
        "        self.train_path = \"train/\"\n",
        "        self.val_path = \"val/\"\n",
        "        self.data = []  # 전처리 전 데이터셋\n",
        "\n",
        "        self.label = [\"_ari\", \"_smallthe\", \"_corki\", \"_velkoz\", \"_thresh\", \"_gragas\", \"_atrox\", \"_kogmaw\", \"_8\", \"_9\"]  # 10개의 이미지 폴더에 대한 값\n",
        "        self.model_data1 = []  # 기본 이미지 모델 데이터셋\n",
        "        self.model_data2 = []  # 캐니 이미지 모델 데이터셋\n",
        "        self.valimgs = []    # 폴더 내 이미지 파일 이름을 저장할 리스트\n",
        "        self.trainimgs = []  # 폴더 내 이미지 파일 이름을 저장할 리스트\n",
        "        self.train_save = 3\n",
        "        self.val_save = 11\n",
        "    def reset_folder(self):\n",
        "        shutil.rmtree(self.train_path)\n",
        "        shutil.rmtree(self.val_path)\n",
        "        print(\"폴더 초기화\")\n",
        "    def set_folder(self):\n",
        "        os.makedirs(self.video_path, exist_ok=True)\n",
        "        os.makedirs(self.train_path + self.nature_img_folder_path, exist_ok=True)\n",
        "        os.makedirs(self.train_path + self.model1_img_folder_path, exist_ok=True)\n",
        "        os.makedirs(self.train_path + self.model2_img_folder_path, exist_ok=True)\n",
        "        os.makedirs(self.val_path + self.nature_img_folder_path, exist_ok=True)\n",
        "        os.makedirs(self.val_path + self.model1_img_folder_path, exist_ok=True)\n",
        "        os.makedirs(self.val_path + self.model2_img_folder_path, exist_ok=True)\n",
        "        for label in self.label:                   # 10개의 하위 폴더 생성 코드 추가 작성\n",
        "            os.makedirs(self.video_path + label, exist_ok=True)\n",
        "            os.makedirs(self.train_path + self.nature_img_folder_path + label, exist_ok=True)\n",
        "            os.makedirs(self.train_path + self.model1_img_folder_path + label, exist_ok=True)\n",
        "            os.makedirs(self.train_path + self.model2_img_folder_path + label, exist_ok=True)\n",
        "            os.makedirs(self.val_path + self.nature_img_folder_path + label, exist_ok=True)\n",
        "            os.makedirs(self.val_path + self.model1_img_folder_path + label, exist_ok=True)\n",
        "            os.makedirs(self.val_path + self.model2_img_folder_path + label, exist_ok=True)\n",
        "        print(\"setting folder\")\n",
        "\n",
        "    def save_train(self):\n",
        "        for label in self.label:\n",
        "            for file_name in os.listdir(self.video_path + label):\n",
        "                file_path = os.path.join(self.video_path + label, file_name)\n",
        "                video_capture = cv2.VideoCapture(file_path) # 동영상 로드\n",
        "                if not video_capture.isOpened():\n",
        "                    print(\"error_00: \" + label + \": 동영상 없음 \")\n",
        "                frame_count, train_count, val_count = 0, 0, 0\n",
        "                fps = video_capture.get(cv2.CAP_PROP_FPS)  # fps 설정\n",
        "                train_save = int(fps * self.train_save/10)  # 훈련에 사용될 이미지 정리  3초 간격으로 작성\n",
        "                val_save = int(fps * self.val_save/10)   # 훈련에 사용될 이미지 정리  11초 간격으로 작성\n",
        "                while True:\n",
        "                    ret, frame = video_capture.read()\n",
        "                    if not ret:\n",
        "                        break  # 프레임이 더 이상 없으면 반복문 종료\n",
        "                    # 프레임 파일 이름 형식 지정 및 저장\n",
        "                    if frame_count % train_save == 0:\n",
        "                        frame_filename = os.path.join(self.train_path + self.nature_img_folder_path + label, f\"{file_name}_{train_count:04d}.png\")\n",
        "                        frame = cv2.resize(frame, (240, 240))  # 이미지 크기 미리 정리\n",
        "                        cv2.imwrite(frame_filename, frame)\n",
        "                        train_count += 1\n",
        "                    elif frame_count % val_save == 0:  # elif 이미지 중복 제거\n",
        "                        frame_filename = os.path.join(self.val_path + self.nature_img_folder_path + label, f\"{file_name}_{val_count:04d}.png\")\n",
        "                        frame = cv2.resize(frame, (240, 240))  # 이미지 크기 미리 정리\n",
        "                        cv2.imwrite(frame_filename, frame)\n",
        "                        val_count += 1\n",
        "                    frame_count += 1\n",
        "                    # 자원 해제\n",
        "                video_capture.release()\n",
        "\n",
        "    # 이미지의 폴더 패스를 받고 이를 self.imgs 에 저장한다.\n",
        "    def set_img(self, model): #이미지 이름을 모두 따서 self.imgs에 저장해둠\n",
        "        self.valimgs = []\n",
        "        self.trainimgs = []\n",
        "        for label in self.label:\n",
        "            for img in os.listdir(self.val_path + model + label):\n",
        "                if os.path.splitext(img)[1].lower() in {\".png\", \".jpg\"}:\n",
        "                    self.valimgs.append([self.val_path, model, label, img])\n",
        "                else:\n",
        "                    print(\"error_01: 이미지 전처리 에러\")\n",
        "        for label in self.label:\n",
        "            for img in os.listdir(self.train_path + model + label):\n",
        "                if os.path.splitext(img)[1].lower() in {\".png\", \".jpg\"}:\n",
        "                    self.trainimgs.append([self.train_path, model, label, img])\n",
        "                else:\n",
        "                    print(\"error_01: 이미지 전처리 에러\")\n",
        "    \"\"\"\n",
        "    # 이미지 로테이트 함수\n",
        "    def made_model1_dataset(self):\n",
        "        self.set_img(self.nature_img_folder_path)\n",
        "        for path in self.valimgs:\n",
        "            image = cv2.imread(path[0]+path[1]+path[2]+\"/\"+path[3], cv2.IMREAD_COLOR)\n",
        "            height, width = image.shape[:2]\n",
        "            for i in range(12):\n",
        "                r_matrix = cv2.getRotationMatrix2D((width // 2, height // 2), i * 30, 1.0)\n",
        "                r_img = cv2.warpAffine(image, r_matrix, (width, height))\n",
        "                cv2.imwrite(self.val_path+self.model1_img_folder_path+path[2]+\"/\"+str(i)+path[3], r_img)\n",
        "        for path in self.trainimgs:\n",
        "            image = cv2.imread(path[0]+path[1]+path[2]+\"/\"+path[3], cv2.IMREAD_COLOR)\n",
        "            height, width = image.shape[:2]\n",
        "            for i in range(12):\n",
        "                r_matrix = cv2.getRotationMatrix2D((width // 2, height // 2), i * 30, 1.0)\n",
        "                r_img = cv2.warpAffine(image, r_matrix, (width, height))\n",
        "                cv2.imwrite(self.train_path+self.model1_img_folder_path+path[2]+\"/\"+str(i)+path[3], r_img)\n",
        "        # return self.model_data1    # data리스트 반환\n",
        "      \"\"\"\n",
        "    def made_model2_dataset(self):\n",
        "        self.set_img(self.nature_img_folder_path)\n",
        "        for path in self.valimgs:\n",
        "            image = cv2.imread(path[0] + path[1] + path[2]+\"/\"+path[3], cv2.IMREAD_GRAYSCALE)\n",
        "            img_c = cv2.Canny(image, 30, 80)\n",
        "            cv2.imwrite(path[0]+self.model2_img_folder_path+path[2]+\"/\"+path[3], img_c)\n",
        "        for path in self.trainimgs:\n",
        "            image = cv2.imread(path[0] + path[1] + path[2] + \"/\" + path[3], cv2.IMREAD_GRAYSCALE)\n",
        "            img_c = cv2.Canny(image, 30, 80)\n",
        "            cv2.imwrite(path[0] + self.model2_img_folder_path + path[2]+\"/\" + path[3], img_c)\n",
        "        print(\"생성 완료\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "IMGP = Image_propressing()\n",
        "IMGP.reset_folder()  # 폴더 초기화\n",
        "IMGP.set_folder()\n",
        "IMGP.save_train()\n",
        "#IMGP.made_model1_dataset() # 로테이트 이미지 생성자 사용 안함\n",
        "#IMGP.made_model2_dataset() # canny이미지 생성자 사용 안함\n",
        "\"\"\"\n",
        "IMGP.train_path = \"train1/\"\n",
        "IMGP.val_path = \"val1/\"\n",
        "IMGP.train_save = 4\n",
        "IMGP.val_save = 13\n",
        "IMGP.set_folder()\n",
        "IMGP.save_train()\n",
        "IMGP.made_model1_dataset()\n",
        "IMGP.made_model2_dataset()\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        },
        "id": "ZwvF52xsqCt9",
        "outputId": "5539b798-8ed5-4275-e858-75bda9227608"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "폴더 초기화\n",
            "setting folder\n",
            "error_00: _ari: 동영상 없음 \n",
            "error_00: _smallthe: 동영상 없음 \n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nIMGP.train_path = \"train1/\"\\nIMGP.val_path = \"val1/\"\\nIMGP.train_save = 4\\nIMGP.val_save = 13\\nIMGP.set_folder()\\nIMGP.save_train()\\nIMGP.made_model1_dataset()\\nIMGP.made_model2_dataset()\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "V1OI3MGjAxrQ"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import models, layers\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.ensemble import VotingClassifier,BaggingClassifier\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.applications import VGG16, Xception , InceptionV3\n",
        "train_dir = \"train/nature\"\n",
        "val_dir = \"val/nature\"\n",
        "# 데이터 증강 (학습 데이터에만 적용)\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=180,       # 회전 범위 증가\n",
        ")\n",
        "\n",
        "# 검증 데이터는 증강하지 않음 (단순히 정규화만 적용)\n",
        "val_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "# 학습 데이터 생성기\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_dir,  # 학습 데이터 경로\n",
        "    target_size=(224, 224),  # VGG16의 입력 크기에 맞게 리사이즈\n",
        "    batch_size=32,\n",
        "    class_mode='categorical',  # 다중 클래스 분류\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "# 검증 데이터 생성기\n",
        "val_generator = val_datagen.flow_from_directory(\n",
        "    val_dir,  # 검증 데이터 경로\n",
        "    target_size=(224, 224),  # VGG16의 입력 크기에 맞게 리사이즈\n",
        "    batch_size=32,\n",
        "    class_mode='categorical'  # 다중 클래스 분류\n",
        ")\n",
        "\n",
        "# tf.data.Dataset으로 변환\n",
        "train_dataset = tf.data.Dataset.from_generator(\n",
        "    lambda: train_generator,\n",
        "    output_signature=(\n",
        "        tf.TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32),\n",
        "        tf.TensorSpec(shape=(None, 10), dtype=tf.float32)\n",
        "    )\n",
        ")\n",
        "\n",
        "val_dataset = tf.data.Dataset.from_generator(\n",
        "    lambda: val_generator,\n",
        "    output_signature=(\n",
        "        tf.TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32),\n",
        "        tf.TensorSpec(shape=(None, 10), dtype=tf.float32)\n",
        "    )\n",
        ")\n",
        "train_dataset = train_dataset.repeat()  # 학습 데이터 무한 반복\n",
        "val_dataset = val_dataset.repeat()      # 검증 데이터 무한 반복\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TDznmrcxaSe1",
        "outputId": "2d5a0185-27eb-4e73-8ec6-4951f3eae432"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 4872 images belonging to 10 classes.\n",
            "Found 852 images belonging to 10 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 보팅에 사용할 모델 불러오기 (pre-trained weights 사용, top layers 제외)\n",
        "vgg16_base = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "inceptionv3_base = InceptionV3(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "xception_base = Xception(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7SiBHS5MiHx9",
        "outputId": "b46e02aa-2744-4d59-dd2a-84e1d861e84d"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "\u001b[1m58889256/58889256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_v3/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "\u001b[1m87910968/87910968\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/xception/xception_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "\u001b[1m83683744/83683744\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def addlayer(model):\n",
        "# 모델에 Flatten 레이어 추가\n",
        "  x = layers.Flatten()(model.output)\n",
        "  # Dense 레이어 추가 (출력 클래스 수에 맞게)\n",
        "  x = layers.Dense(512, activation='relu')(x)\n",
        "  x = layers.Dropout(0.6)(x)  # Dropout 추가\n",
        "  x = layers.Dense(10, activation='softmax')(x)\n",
        "  made_model = models.Model(inputs=model.input, outputs=x)\n",
        "  return made_model"
      ],
      "metadata": {
        "id": "sXS_geIIhcQd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vgg16 = addlayer(vgg16_base)\n",
        "inceptionv3 = addlayer(inceptionv3_base)\n",
        "xception = addlayer(xception_base)\n",
        "\n",
        "for layer in vgg16_base.layers:\n",
        "    layer.trainable = False\n",
        "for layer in inceptionv3_base.layers:\n",
        "    layer.trainable = False\n",
        "for layer in xception_base.layers:\n",
        "    layer.trainable = False\n",
        "# 모델 컴파일\n",
        "vgg16.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "inceptionv3.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "xception.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "# 모델 요약 정보 출력\n",
        "#vgg16.summary()\n",
        "#ResNet50.summary()\n",
        "#inceptionv3.summary()\n",
        "#xception.summary()"
      ],
      "metadata": {
        "id": "_yW-VKqGhehL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# steps_per_epoch, validation_steps 설정\n",
        "steps_per_epoch = train_generator.samples // train_generator.batch_size\n",
        "validation_steps = val_generator.samples // val_generator.batch_size\n",
        "# 모델 학습\n",
        "def hist(model,train_dataset,steps_per_epoch,val_dataset,validation_steps):\n",
        "  history = model.fit(\n",
        "      train_dataset,  # tf.data.Dataset 사용\n",
        "      steps_per_epoch=steps_per_epoch,  # 배치 크기에 맞춰 학습 스텝 설정\n",
        "      epochs=50,  # 에포크 수\n",
        "      validation_data=val_dataset,  # 검증 데이터\n",
        "      validation_steps=validation_steps  # 검증 배치 크기 맞추기\n",
        "  )\n",
        "  return history\n",
        "#vgg16_h =  hist(vgg16,train_dataset,steps_per_epoch,val_dataset,validation_steps)\n",
        "inceptionv3 =  hist(vgg16,train_dataset,steps_per_epoch,val_dataset,validation_steps)\n",
        "inceptionv3.save('inceptionv3.keras')\n",
        "xception_h =  hist(vgg16,train_dataset,steps_per_epoch,val_dataset,validation_steps)\n",
        "xception.save('xception.keras')\n",
        "# 모델 저장\n",
        "#vgg16.save('vgg16.keras')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MSsRx7bPdn22",
        "outputId": "7d629c15-6f28-44a3-cd23-8d46377385b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3283s\u001b[0m 22s/step - accuracy: 0.4416 - loss: 2.0022 - val_accuracy: 0.7392 - val_loss: 0.8275\n",
            "Epoch 2/20\n",
            "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3188s\u001b[0m 21s/step - accuracy: 0.6282 - loss: 0.8885 - val_accuracy: 0.8183 - val_loss: 0.5758\n",
            "Epoch 3/20\n",
            "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3210s\u001b[0m 21s/step - accuracy: 0.6981 - loss: 0.7171 - val_accuracy: 0.8598 - val_loss: 0.5255\n",
            "Epoch 4/20\n",
            "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3172s\u001b[0m 21s/step - accuracy: 0.7135 - loss: 0.6829 - val_accuracy: 0.8634 - val_loss: 0.4298\n",
            "Epoch 5/20\n",
            "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3177s\u001b[0m 21s/step - accuracy: 0.7487 - loss: 0.6093 - val_accuracy: 0.8829 - val_loss: 0.3687\n",
            "Epoch 6/20\n",
            "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3176s\u001b[0m 21s/step - accuracy: 0.7585 - loss: 0.5762 - val_accuracy: 0.8854 - val_loss: 0.3352\n",
            "Epoch 7/20\n",
            "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3173s\u001b[0m 21s/step - accuracy: 0.7732 - loss: 0.5490 - val_accuracy: 0.9256 - val_loss: 0.3005\n",
            "Epoch 8/20\n",
            "\u001b[1m 61/152\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m27:19\u001b[0m 18s/step - accuracy: 0.7960 - loss: 0.5024"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "history_list = [vgg16_h,  inceptionv3_h, xception_h]\n",
        "model_names = ['VGG16',  'InceptionV3', 'Xception']\n",
        "\n",
        "# 각 모델에 대해 학습 및 검증 정확도와 손실을 그리는 반복문\n",
        "for i, history in enumerate(history_list):\n",
        "    plt.figure(figsize=(12, 5))\n",
        "\n",
        "    # 정확도 그래프\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(history.history['accuracy'])\n",
        "    plt.plot(history.history['val_accuracy'])\n",
        "    plt.title(f'{model_names[i]} - Accuracy')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "\n",
        "    # 손실 그래프\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(history.history['loss'])\n",
        "    plt.plot(history.history['val_loss'])\n",
        "    plt.title(f'{model_names[i]} - Loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "I6Uerrv6n_wA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import VotingClassifier\n",
        "named_estimators = [\n",
        "    (\"inceptionv3\", inceptionv3),\n",
        "    (\"xception_h\",  xception_h),\n",
        "    (\"vgg16\", vgg16),\n",
        "]\n",
        "voting_clf = VotingClassifier(named_estimators)"
      ],
      "metadata": {
        "id": "mF_iWbAEdQVl"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
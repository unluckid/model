{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jX7ukQPOAxrM"
      },
      "source": [
        "<table align=\"left\">\n",
        "  <td>\n",
        "    <a href=\"https://colab.research.google.com/github/rickiepark/handson-ml3/blob/main/12_custom_models_and_training_with_tensorflow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sFcMpxP2AxrN",
        "tags": []
      },
      "source": [
        "# 설정"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "xNKQVwanAxrN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        },
        "outputId": "675bac04-8273-4c57-9b69-f889efa7a20e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=181xBl-rU9gBAL8-iEZbPNxKzPZY-QCGu\n",
            "From (redirected): https://drive.google.com/uc?id=181xBl-rU9gBAL8-iEZbPNxKzPZY-QCGu&confirm=t&uuid=b86dba6b-dd00-4ae0-b8c1-568901641bee\n",
            "To: /content/dataset.zip\n",
            "100%|██████████| 1.65G/1.65G [00:25<00:00, 64.3MB/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/dataset.zip'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import gdown\n",
        "import numpy as np\n",
        "\n",
        "file_id = '181xBl-rU9gBAL8-iEZbPNxKzPZY-QCGu'\n",
        "download_url = f'https://drive.google.com/uc?id={file_id}'\n",
        "\n",
        "\n",
        "destination_path = '/content/dataset.zip'  # 파일 경로 설정\n",
        "\n",
        "# 다운로드 시작\n",
        "gdown.download(download_url, destination_path, quiet=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "0Ddo9OPsAxrO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64eb7a45-d2d7-43af-aec3-26a747bd9b19"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "압축이 /content/에 해제되었습니다.\n"
          ]
        }
      ],
      "source": [
        "import zipfile\n",
        "\n",
        "\n",
        "zip_file_path = '/content/dataset.zip'\n",
        "extract_path = '/content/'\n",
        "\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n",
        "\n",
        "print(f\"압축이 {extract_path}에 해제되었습니다.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "JN8_1_vCAxrP"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "class Image_propressing:\n",
        "    def __init__(self):\n",
        "        #  폴더 경로\n",
        "        self.video_path = \"video/\"  # 비디오 폴더\n",
        "        self.nature_img_folder_path = \"nature/\"  # 전처리 이전 폴더\n",
        "        self.model1_img_folder_path = \"model1/\"  # 기본 모델 폴더\n",
        "        self.model2_img_folder_path = \"model2/\"  # 캐니 모델 폴더\n",
        "        self.train_path = \"train/\"\n",
        "        self.val_path = \"val/\"\n",
        "        self.data = []  # 전처리 전 데이터셋\n",
        "\n",
        "        self.label = [\"_ari\", \"_smallthe\", \"_corki\", \"_velkoz\", \"_thresh\", \"_gragas\", \"_atrox\", \"_kogmaw\", \"_8\", \"_9\"]  # 10개의 이미지 폴더에 대한 값\n",
        "        self.model_data1 = []  # 기본 이미지 모델 데이터셋\n",
        "        self.model_data2 = []  # 캐니 이미지 모델 데이터셋\n",
        "        self.valimgs = []    # 폴더 내 이미지 파일 이름을 저장할 리스트\n",
        "        self.trainimgs = []  # 폴더 내 이미지 파일 이름을 저장할 리스트\n",
        "        self.train_save = 3\n",
        "        self.val_save = 11\n",
        "    def reset_folder(self):\n",
        "        shutil.rmtree(self.train_path)\n",
        "        shutil.rmtree(self.val_path)\n",
        "        print(\"폴더 초기화\")\n",
        "    def set_folder(self):\n",
        "        os.makedirs(self.video_path, exist_ok=True)\n",
        "        os.makedirs(self.train_path + self.nature_img_folder_path, exist_ok=True)\n",
        "        os.makedirs(self.train_path + self.model1_img_folder_path, exist_ok=True)\n",
        "        os.makedirs(self.train_path + self.model2_img_folder_path, exist_ok=True)\n",
        "        os.makedirs(self.val_path + self.nature_img_folder_path, exist_ok=True)\n",
        "        os.makedirs(self.val_path + self.model1_img_folder_path, exist_ok=True)\n",
        "        os.makedirs(self.val_path + self.model2_img_folder_path, exist_ok=True)\n",
        "        for label in self.label:                   # 10개의 하위 폴더 생성 코드 추가 작성\n",
        "            os.makedirs(self.video_path + label, exist_ok=True)\n",
        "            os.makedirs(self.train_path + self.nature_img_folder_path + label, exist_ok=True)\n",
        "            os.makedirs(self.train_path + self.model1_img_folder_path + label, exist_ok=True)\n",
        "            os.makedirs(self.train_path + self.model2_img_folder_path + label, exist_ok=True)\n",
        "            os.makedirs(self.val_path + self.nature_img_folder_path + label, exist_ok=True)\n",
        "            os.makedirs(self.val_path + self.model1_img_folder_path + label, exist_ok=True)\n",
        "            os.makedirs(self.val_path + self.model2_img_folder_path + label, exist_ok=True)\n",
        "        print(\"setting folder\")\n",
        "\n",
        "    def save_train(self):\n",
        "        for label in self.label:\n",
        "            for file_name in os.listdir(self.video_path + label):\n",
        "                file_path = os.path.join(self.video_path + label, file_name)\n",
        "                video_capture = cv2.VideoCapture(file_path) # 동영상 로드\n",
        "                if not video_capture.isOpened():\n",
        "                    print(\"error_00: \" + label + \": 동영상 없음 \")\n",
        "                frame_count, train_count, val_count = 0, 0, 0\n",
        "                fps = video_capture.get(cv2.CAP_PROP_FPS)  # fps 설정\n",
        "                train_save = int(fps * self.train_save)  # 훈련에 사용될 이미지 정리  3초 간격으로 작성\n",
        "                val_save = int(fps * self.val_save)   # 훈련에 사용될 이미지 정리  11초 간격으로 작성\n",
        "                while True:\n",
        "                    ret, frame = video_capture.read()\n",
        "                    if not ret:\n",
        "                        break  # 프레임이 더 이상 없으면 반복문 종료\n",
        "                    # 프레임 파일 이름 형식 지정 및 저장\n",
        "                    if frame_count % train_save == 0:\n",
        "                        frame_filename = os.path.join(self.train_path + self.nature_img_folder_path + label, f\"{file_name}_{train_count:04d}.png\")\n",
        "                        frame = cv2.resize(frame, (240, 240))  # 이미지 크기 미리 정리\n",
        "                        cv2.imwrite(frame_filename, frame)\n",
        "                        train_count += 1\n",
        "                    elif frame_count % val_save == 0:  # elif 이미지 중복 제거\n",
        "                        frame_filename = os.path.join(self.val_path + self.nature_img_folder_path + label, f\"{file_name}_{val_count:04d}.png\")\n",
        "                        frame = cv2.resize(frame, (240, 240))  # 이미지 크기 미리 정리\n",
        "                        cv2.imwrite(frame_filename, frame)\n",
        "                        val_count += 1\n",
        "                    frame_count += 1\n",
        "                    # 자원 해제\n",
        "                video_capture.release()\n",
        "\n",
        "    # 이미지의 폴더 패스를 받고 이를 self.imgs 에 저장한다.\n",
        "    def set_img(self, model): #이미지 이름을 모두 따서 self.imgs에 저장해둠\n",
        "        self.valimgs = []\n",
        "        self.trainimgs = []\n",
        "        for label in self.label:\n",
        "            for img in os.listdir(self.val_path + model + label):\n",
        "                if os.path.splitext(img)[1].lower() in {\".png\", \".jpg\"}:\n",
        "                    self.valimgs.append([self.val_path, model, label, img])\n",
        "                else:\n",
        "                    print(\"error_01: 이미지 전처리 에러\")\n",
        "        for label in self.label:\n",
        "            for img in os.listdir(self.train_path + model + label):\n",
        "                if os.path.splitext(img)[1].lower() in {\".png\", \".jpg\"}:\n",
        "                    self.trainimgs.append([self.train_path, model, label, img])\n",
        "                else:\n",
        "                    print(\"error_01: 이미지 전처리 에러\")\n",
        "    \"\"\" # 이미지 로테이트 함수\n",
        "    def made_model1_dataset(self):\n",
        "        self.set_img(self.nature_img_folder_path)\n",
        "        for path in self.valimgs:\n",
        "            image = cv2.imread(path[0]+path[1]+path[2]+\"/\"+path[3], cv2.IMREAD_COLOR)\n",
        "            height, width = image.shape[:2]\n",
        "            for i in range(12):\n",
        "                r_matrix = cv2.getRotationMatrix2D((width // 2, height // 2), i * 30, 1.0)\n",
        "                r_img = cv2.warpAffine(image, r_matrix, (width, height))\n",
        "                cv2.imwrite(self.val_path+self.model1_img_folder_path+path[2]+\"/\"+str(i)+path[3], r_img)\n",
        "        for path in self.trainimgs:\n",
        "            image = cv2.imread(path[0]+path[1]+path[2]+\"/\"+path[3], cv2.IMREAD_COLOR)\n",
        "            height, width = image.shape[:2]\n",
        "            for i in range(12):\n",
        "                r_matrix = cv2.getRotationMatrix2D((width // 2, height // 2), i * 30, 1.0)\n",
        "                r_img = cv2.warpAffine(image, r_matrix, (width, height))\n",
        "                cv2.imwrite(self.train_path+self.model1_img_folder_path+path[2]+\"/\"+str(i)+path[3], r_img)\n",
        "        # return self.model_data1    # data리스트 반환\n",
        "    \"\"\"\n",
        "    def made_model2_dataset(self):\n",
        "        self.set_img(self.model1_img_folder_path)\n",
        "        for path in self.valimgs:\n",
        "            image = cv2.imread(path[0] + path[1] + path[2]+\"/\"+path[3], cv2.IMREAD_GRAYSCALE)\n",
        "            img_c = cv2.Canny(image, 30, 80)\n",
        "            cv2.imwrite(path[0]+self.model2_img_folder_path+path[2]+\"/\"+path[3], img_c)\n",
        "        for path in self.trainimgs:\n",
        "            image = cv2.imread(path[0] + path[1] + path[2] + \"/\" + path[3], cv2.IMREAD_GRAYSCALE)\n",
        "            img_c = cv2.Canny(image, 30, 80)\n",
        "            cv2.imwrite(path[0] + self.model2_img_folder_path + path[2]+\"/\" + path[3], img_c)\n",
        "        print(\"생성 완료\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "IMGP = Image_propressing()\n",
        "IMGP.reset_folder()  # 폴더 초기화\n",
        "IMGP.set_folder()\n",
        "IMGP.save_train()\n",
        "#IMGP.made_model1_dataset()\n",
        "IMGP.made_model2_dataset()\n",
        "\"\"\"\n",
        "IMGP.train_path = \"train1/\"\n",
        "IMGP.val_path = \"val1/\"\n",
        "IMGP.train_save = 4\n",
        "IMGP.val_save = 13\n",
        "IMGP.set_folder()\n",
        "IMGP.save_train()\n",
        "IMGP.made_model1_dataset()\n",
        "IMGP.made_model2_dataset()\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        },
        "id": "ZwvF52xsqCt9",
        "outputId": "eba8f58e-5b28-4537-8184-f69a83145894"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "폴더 초기화\n",
            "setting folder\n",
            "error_00: _ari: 동영상 없음 \n",
            "error_00: _smallthe: 동영상 없음 \n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nIMGP.train_path = \"train1/\"\\nIMGP.val_path = \"val1/\"\\nIMGP.train_save = 4\\nIMGP.val_save = 13\\nIMGP.set_folder()\\nIMGP.save_train()\\nIMGP.made_model1_dataset()\\nIMGP.made_model2_dataset()\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "V1OI3MGjAxrQ"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import models, layers\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.ensemble import VotingClassifier,BaggingClassifier\n",
        "\n",
        "class Model:\n",
        "    def __init__(self):\n",
        "        self.models = []\n",
        "        self.model  = None\n",
        "        self.model1 = None\n",
        "        self.model2 = None\n",
        "        self.train_dir = \"train/nature\"\n",
        "        self.val_dir = \"val/nature1\"\n",
        "        self.train_dir1 = \"train/model2\"\n",
        "        self.val_dir1 = \"val/model2\"\n",
        "        self.bagging_model1 = None\n",
        "        self.bagging_model2 = None\n",
        "        self.train_dataset = None\n",
        "        self.val_dataset = None\n",
        "        self.color =\"rgb\"\n",
        "        self.gray = \"gray\"\n",
        "        self.epoch = 10\n",
        "    def setmodel1(self):\n",
        "        self.model1 = models.Sequential()\n",
        "        self.model1.add( layers.Conv2D(filters = 32,kernel_size = (3,3), activation= 'relu', input_shape=(240,240,3), strides = (1,1), padding = 'same')) #128\n",
        "        self.model1.add( layers.BatchNormalization())\n",
        "        self.model1.add( layers.MaxPooling2D(2,2))\n",
        "        self.model1.add( layers.Conv2D(filters = 64,kernel_size =(3,3), activation='relu'))   #64\n",
        "        self.model1.add( layers.BatchNormalization())\n",
        "        self.model1.add( layers.MaxPooling2D(2,2))\n",
        "        self.model1.add( layers.Conv2D(filters = 32,  kernel_size =(5,5), activation= 'relu'))   #32\n",
        "        self.model1.add( layers.BatchNormalization())\n",
        "        self.model1.add( layers.MaxPooling2D(2,2))\n",
        "        self.model1.add( layers.Flatten())\n",
        "        self.model1.add( layers.Dense(units=512, activation= 'relu'))\n",
        "        self.model1.add( layers.Dropout(0.5))\n",
        "        self.model1.add(layers.Dense(units=10, activation='softmax'))\n",
        "        self.model1.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "        print(\"모델1 구조 설정\")\n",
        "        return self.model1\n",
        "\n",
        "    def setmodel2(self):\n",
        "        self.model2 = models.Sequential()\n",
        "\n",
        "        self.model2.add(\n",
        "            layers.Conv2D(filters=32, kernel_size=(3, 3), activation='relu', input_shape=(240, 240, 1),\n",
        "                          strides=(1, 1), padding='same'))  # 128\n",
        "        self.model2.add(layers.BatchNormalization())\n",
        "        self.model2.add(layers.MaxPooling2D(2, 2))\n",
        "        self.model2.add(layers.Conv2D(filters=64, kernel_size=(3, 3), activation='relu')) # 64\n",
        "        self.model2.add(layers.BatchNormalization())\n",
        "        self.model2.add(layers.MaxPooling2D(2, 2))\n",
        "        self.model2.add(layers.Conv2D(filters=32, kernel_size=(5, 5), activation='relu'))  # 32\n",
        "        self.model2.add(layers.BatchNormalization())\n",
        "        self.model2.add(layers.MaxPooling2D(2, 2))\n",
        "        self.model2.add(layers.Flatten())\n",
        "        self.model2.add(layers.Dense(units=512, activation='relu'))\n",
        "        self.model2.add(layers.Dropout(0.5))\n",
        "        self.model2.add(layers.Dense(units=10, activation='softmax'))\n",
        "        self.model2.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "        print(\"모델2 구조 설정\")\n",
        "        return self.model2\n",
        "    def made_data(self, train_dir, val_dir, color):\n",
        "        train_datagen = ImageDataGenerator(\n",
        "            rescale = 1./255, zoom_range = 0.3,rotation_range=180)# 학습시마다 변경된 이미지\n",
        "        val_datagen = ImageDataGenerator(rotation_range=180, rescale = 1./255)\n",
        "        if color == \"rgb\":\n",
        "            train_generator = train_datagen.flow_from_directory(\n",
        "                train_dir,\n",
        "                target_size=(240, 240),\n",
        "                batch_size=16,\n",
        "                class_mode = 'categorical',\n",
        "                color_mode='rgb')\n",
        "            val_generator = val_datagen.flow_from_directory(\n",
        "                val_dir,\n",
        "                target_size=(240, 240),\n",
        "                batch_size=16,\n",
        "                class_mode = 'categorical',color_mode='rgb') #binary\n",
        "            self.train_dataset = tf.data.Dataset.from_generator(\n",
        "            lambda: train_generator,\n",
        "            output_signature=(\n",
        "                tf.TensorSpec(shape=(None, 240, 240, 3), dtype=tf.float32),\n",
        "                tf.TensorSpec(shape=(None, 10), dtype=tf.float32)))\n",
        "\n",
        "            self.val_dataset = tf.data.Dataset.from_generator(\n",
        "                lambda: val_generator,\n",
        "                output_signature=(\n",
        "                    tf.TensorSpec(shape=(None, 240, 240, 3), dtype=tf.float32),\n",
        "                    tf.TensorSpec(shape=(None, 10), dtype=tf.float32)))\n",
        "        elif color == \"gray\":\n",
        "            train_generator = train_datagen.flow_from_directory(\n",
        "                train_dir,\n",
        "                target_size=(240, 240),\n",
        "                batch_size=16,\n",
        "                class_mode='categorical',\n",
        "                color_mode='grayscale')\n",
        "            val_generator = val_datagen.flow_from_directory(\n",
        "                val_dir,\n",
        "                target_size=(240, 240),\n",
        "                batch_size=16,\n",
        "                class_mode='categorical', color_mode='grayscale')  # binary\n",
        "\n",
        "            self.train_dataset = tf.data.Dataset.from_generator(\n",
        "            lambda: train_generator,\n",
        "            output_signature=(\n",
        "                tf.TensorSpec(shape=(None, 240, 240, 1), dtype=tf.float32),\n",
        "                tf.TensorSpec(shape=(None, 10), dtype=tf.float32)))\n",
        "\n",
        "            self.val_dataset = tf.data.Dataset.from_generator(\n",
        "                lambda: val_generator,\n",
        "                output_signature=(\n",
        "                    tf.TensorSpec(shape=(None, 240, 240, 1), dtype=tf.float32),\n",
        "                    tf.TensorSpec(shape=(None, 10), dtype=tf.float32)))\n",
        "            # tf.data.Dataset으로 변환\n",
        "        self.steps_per_epoch = train_generator.samples // train_generator.batch_size\n",
        "        self.validation_steps = val_generator.samples // val_generator.batch_size\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def history(self):\n",
        "        plt.plot(self.history_model1.history['accuracy'])\n",
        "        plt.plot(self.history_model1.history['val_accuracy'])\n",
        "        plt.plot(self.history_model2.history['accuracy'])\n",
        "        plt.plot(self.history_model2.history['val_accuracy'])\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.xlabel('Accuracy')\n",
        "        plt.legend(['Train_model1', 'Test_model1','Train_model2', 'Test_model2'], loc='upper left')\n",
        "        plt.show()\n",
        "\n",
        "    def set_models(self):\n",
        "        self.models.append(self.model1)\n",
        "        self.models.append(self.model2)\n",
        "        print(\"모델 저장\")\n",
        "    def save_model(self, model,filepath):\n",
        "        model.save(filepath)\n",
        "        print(filepath + \"모델 저장\")\n",
        "    def made_voting_model(self, model1, model2):\n",
        "        self.model = [model1, model2]\n",
        "    def made_model(self,model):\n",
        "        if model == \"natural\":\n",
        "            # 앙상블 모델 함수 활성화 추가 코드\n",
        "            self.model1 = self.setmodel1()\n",
        "\n",
        "            self.made_data(self.train_dir, self.val_dir, self.color) #model1\n",
        "            self.history_model1 = self.model1.fit(self.train_dataset, epochs=self.epoch,steps_per_epoch= self.steps_per_epoch, validation_data= self.val_dataset,validation_steps=self.validation_steps)\n",
        "            return self.model1\n",
        "        elif model == \"canny\":\n",
        "            self.model2 = self.setmodel2()\n",
        "            self.made_data(self.train_dir1, self.val_dir1, self.gray) #model2\n",
        "            self.history_model2 = self.model2.fit( self.train_dataset, epochs=self.epoch,steps_per_epoch= self.steps_per_epoch, validation_data= self.val_dataset, validation_steps=self.validation_steps)\n",
        "            return self.model2\n",
        "        self.history()\n",
        "\n",
        "    def predict(self, model):\n",
        "        pred = model.predict()\n",
        "        return pred\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 522
        },
        "id": "U5M77FruAxrQ",
        "outputId": "7f2ca6eb-acc5-4a19-b95c-85c4cbed7f9e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "모델2 구조 설정\n",
            "Found 0 images belonging to 10 classes.\n",
            "Found 0 images belonging to 10 classes.\n",
            "Epoch 1/30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   8127/Unknown \u001b[1m30s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-b6281dec9841>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmade_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"canny\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"canny.h5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodel1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmade_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"natural\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-31-3bcafd80a8c8>\u001b[0m in \u001b[0;36mmade_model\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m    151\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetmodel2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmade_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_dir1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mval_dir1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgray\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#model2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory_model2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mval_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    318\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mepoch_iterator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menumerate_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 320\u001b[0;31m                     \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    321\u001b[0m                     \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pythonify_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    831\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 833\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    834\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    876\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    877\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 878\u001b[0;31m       results = tracing_compilation.call_function(\n\u001b[0m\u001b[1;32m    879\u001b[0m           \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    880\u001b[0m       )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m   \u001b[0mbound_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m   \u001b[0mflat_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbound_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m   return function._call_flat(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    140\u001b[0m       \u001b[0mflat_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m   )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1320\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1321\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1324\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;34m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m     \u001b[0mflat_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_recording\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bound_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m             outputs = self._bound_context.call_function(\n\u001b[0m\u001b[1;32m    252\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1550\u001b[0m     \u001b[0mcancellation_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcancellation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcancellation_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1552\u001b[0;31m       outputs = execute.execute(\n\u001b[0m\u001b[1;32m   1553\u001b[0m           \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1554\u001b[0m           \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "model = Model()\n",
        "model.epoch = 30\n",
        "model2 = model.made_model(\"canny\")\n",
        "model.save_model(model2, \"canny.h5\")\n",
        "model1 = model.made_model(\"natural\")\n",
        "model.save_model(model1, \"model.h5\")\n",
        "voting_model1 = model.made_voting_model( model1, model2)\n",
        "# 배깅\n",
        "\"\"\"\n",
        "model.train_dir = \"train1/model1\"\n",
        "model.val_dir = \"val1/model1\"\n",
        "model.train_dir1 = \"train1/model2\"\n",
        "model.val_dir1 = \"val1/model2\"\n",
        "model3 = model.made_model(\"canny\")\n",
        "model4 = model.made_model(\"natural\")\n",
        "voting_model2 = model.made_voting_model()\n",
        "\"\"\"\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
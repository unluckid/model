{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jX7ukQPOAxrM"
      },
      "source": [
        "<table align=\"left\">\n",
        "  <td>\n",
        "    <a href=\"https://colab.research.google.com/github/rickiepark/handson-ml3/blob/main/12_custom_models_and_training_with_tensorflow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sFcMpxP2AxrN",
        "tags": []
      },
      "source": [
        "# 설정"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "xNKQVwanAxrN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        },
        "outputId": "675bac04-8273-4c57-9b69-f889efa7a20e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=181xBl-rU9gBAL8-iEZbPNxKzPZY-QCGu\n",
            "From (redirected): https://drive.google.com/uc?id=181xBl-rU9gBAL8-iEZbPNxKzPZY-QCGu&confirm=t&uuid=b86dba6b-dd00-4ae0-b8c1-568901641bee\n",
            "To: /content/dataset.zip\n",
            "100%|██████████| 1.65G/1.65G [00:25<00:00, 64.3MB/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/dataset.zip'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import gdown\n",
        "import numpy as np\n",
        "\n",
        "file_id = '181xBl-rU9gBAL8-iEZbPNxKzPZY-QCGu'\n",
        "download_url = f'https://drive.google.com/uc?id={file_id}'\n",
        "\n",
        "\n",
        "destination_path = '/content/dataset.zip'  # 파일 경로 설정\n",
        "\n",
        "# 다운로드 시작\n",
        "gdown.download(download_url, destination_path, quiet=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "0Ddo9OPsAxrO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64eb7a45-d2d7-43af-aec3-26a747bd9b19"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "압축이 /content/에 해제되었습니다.\n"
          ]
        }
      ],
      "source": [
        "import zipfile\n",
        "\n",
        "\n",
        "zip_file_path = '/content/dataset.zip'\n",
        "extract_path = '/content/'\n",
        "\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n",
        "\n",
        "print(f\"압축이 {extract_path}에 해제되었습니다.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "JN8_1_vCAxrP"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "class Image_propressing:\n",
        "    def __init__(self):\n",
        "        #  폴더 경로\n",
        "        self.video_path = \"video/\"  # 비디오 폴더\n",
        "        self.nature_img_folder_path = \"nature/\"  # 전처리 이전 폴더\n",
        "        self.model1_img_folder_path = \"model1/\"  # 기본 모델 폴더\n",
        "        self.model2_img_folder_path = \"model2/\"  # 캐니 모델 폴더\n",
        "        self.train_path = \"train/\"\n",
        "        self.val_path = \"val/\"\n",
        "        self.data = []  # 전처리 전 데이터셋\n",
        "\n",
        "        self.label = [\"_ari\", \"_smallthe\", \"_corki\", \"_velkoz\", \"_thresh\", \"_gragas\", \"_atrox\", \"_kogmaw\", \"_8\", \"_9\"]  # 10개의 이미지 폴더에 대한 값\n",
        "        self.model_data1 = []  # 기본 이미지 모델 데이터셋\n",
        "        self.model_data2 = []  # 캐니 이미지 모델 데이터셋\n",
        "        self.valimgs = []    # 폴더 내 이미지 파일 이름을 저장할 리스트\n",
        "        self.trainimgs = []  # 폴더 내 이미지 파일 이름을 저장할 리스트\n",
        "        self.train_save = 3\n",
        "        self.val_save = 11\n",
        "    def reset_folder(self):\n",
        "        shutil.rmtree(self.train_path)\n",
        "        shutil.rmtree(self.val_path)\n",
        "        print(\"폴더 초기화\")\n",
        "    def set_folder(self):\n",
        "        os.makedirs(self.video_path, exist_ok=True)\n",
        "        os.makedirs(self.train_path + self.nature_img_folder_path, exist_ok=True)\n",
        "        os.makedirs(self.train_path + self.model1_img_folder_path, exist_ok=True)\n",
        "        os.makedirs(self.train_path + self.model2_img_folder_path, exist_ok=True)\n",
        "        os.makedirs(self.val_path + self.nature_img_folder_path, exist_ok=True)\n",
        "        os.makedirs(self.val_path + self.model1_img_folder_path, exist_ok=True)\n",
        "        os.makedirs(self.val_path + self.model2_img_folder_path, exist_ok=True)\n",
        "        for label in self.label:                   # 10개의 하위 폴더 생성 코드 추가 작성\n",
        "            os.makedirs(self.video_path + label, exist_ok=True)\n",
        "            os.makedirs(self.train_path + self.nature_img_folder_path + label, exist_ok=True)\n",
        "            os.makedirs(self.train_path + self.model1_img_folder_path + label, exist_ok=True)\n",
        "            os.makedirs(self.train_path + self.model2_img_folder_path + label, exist_ok=True)\n",
        "            os.makedirs(self.val_path + self.nature_img_folder_path + label, exist_ok=True)\n",
        "            os.makedirs(self.val_path + self.model1_img_folder_path + label, exist_ok=True)\n",
        "            os.makedirs(self.val_path + self.model2_img_folder_path + label, exist_ok=True)\n",
        "        print(\"setting folder\")\n",
        "\n",
        "    def save_train(self):\n",
        "        for label in self.label:\n",
        "            for file_name in os.listdir(self.video_path + label):\n",
        "                file_path = os.path.join(self.video_path + label, file_name)\n",
        "                video_capture = cv2.VideoCapture(file_path) # 동영상 로드\n",
        "                if not video_capture.isOpened():\n",
        "                    print(\"error_00: \" + label + \": 동영상 없음 \")\n",
        "                frame_count, train_count, val_count = 0, 0, 0\n",
        "                fps = video_capture.get(cv2.CAP_PROP_FPS)  # fps 설정\n",
        "                train_save = int(fps * self.train_save/10)  # 훈련에 사용될 이미지 정리  3초 간격으로 작성\n",
        "                val_save = int(fps * self.val_save/10)   # 훈련에 사용될 이미지 정리  11초 간격으로 작성\n",
        "                while True:\n",
        "                    ret, frame = video_capture.read()\n",
        "                    if not ret:\n",
        "                        break  # 프레임이 더 이상 없으면 반복문 종료\n",
        "                    # 프레임 파일 이름 형식 지정 및 저장\n",
        "                    if frame_count % train_save == 0:\n",
        "                        frame_filename = os.path.join(self.train_path + self.nature_img_folder_path + label, f\"{file_name}_{train_count:04d}.png\")\n",
        "                        frame = cv2.resize(frame, (240, 240))  # 이미지 크기 미리 정리\n",
        "                        cv2.imwrite(frame_filename, frame)\n",
        "                        train_count += 1\n",
        "                    elif frame_count % val_save == 0:  # elif 이미지 중복 제거\n",
        "                        frame_filename = os.path.join(self.val_path + self.nature_img_folder_path + label, f\"{file_name}_{val_count:04d}.png\")\n",
        "                        frame = cv2.resize(frame, (240, 240))  # 이미지 크기 미리 정리\n",
        "                        cv2.imwrite(frame_filename, frame)\n",
        "                        val_count += 1\n",
        "                    frame_count += 1\n",
        "                    # 자원 해제\n",
        "                video_capture.release()\n",
        "\n",
        "    # 이미지의 폴더 패스를 받고 이를 self.imgs 에 저장한다.\n",
        "    def set_img(self, model): #이미지 이름을 모두 따서 self.imgs에 저장해둠\n",
        "        self.valimgs = []\n",
        "        self.trainimgs = []\n",
        "        for label in self.label:\n",
        "            for img in os.listdir(self.val_path + model + label):\n",
        "                if os.path.splitext(img)[1].lower() in {\".png\", \".jpg\"}:\n",
        "                    self.valimgs.append([self.val_path, model, label, img])\n",
        "                else:\n",
        "                    print(\"error_01: 이미지 전처리 에러\")\n",
        "        for label in self.label:\n",
        "            for img in os.listdir(self.train_path + model + label):\n",
        "                if os.path.splitext(img)[1].lower() in {\".png\", \".jpg\"}:\n",
        "                    self.trainimgs.append([self.train_path, model, label, img])\n",
        "                else:\n",
        "                    print(\"error_01: 이미지 전처리 에러\")\n",
        "    \"\"\"\n",
        "    # 이미지 로테이트 함수\n",
        "    def made_model1_dataset(self):\n",
        "        self.set_img(self.nature_img_folder_path)\n",
        "        for path in self.valimgs:\n",
        "            image = cv2.imread(path[0]+path[1]+path[2]+\"/\"+path[3], cv2.IMREAD_COLOR)\n",
        "            height, width = image.shape[:2]\n",
        "            for i in range(12):\n",
        "                r_matrix = cv2.getRotationMatrix2D((width // 2, height // 2), i * 30, 1.0)\n",
        "                r_img = cv2.warpAffine(image, r_matrix, (width, height))\n",
        "                cv2.imwrite(self.val_path+self.model1_img_folder_path+path[2]+\"/\"+str(i)+path[3], r_img)\n",
        "        for path in self.trainimgs:\n",
        "            image = cv2.imread(path[0]+path[1]+path[2]+\"/\"+path[3], cv2.IMREAD_COLOR)\n",
        "            height, width = image.shape[:2]\n",
        "            for i in range(12):\n",
        "                r_matrix = cv2.getRotationMatrix2D((width // 2, height // 2), i * 30, 1.0)\n",
        "                r_img = cv2.warpAffine(image, r_matrix, (width, height))\n",
        "                cv2.imwrite(self.train_path+self.model1_img_folder_path+path[2]+\"/\"+str(i)+path[3], r_img)\n",
        "        # return self.model_data1    # data리스트 반환\n",
        "      \"\"\"\n",
        "    def made_model2_dataset(self):\n",
        "        self.set_img(self.nature_img_folder_path)\n",
        "        for path in self.valimgs:\n",
        "            image = cv2.imread(path[0] + path[1] + path[2]+\"/\"+path[3], cv2.IMREAD_GRAYSCALE)\n",
        "            img_c = cv2.Canny(image, 30, 80)\n",
        "            cv2.imwrite(path[0]+self.model2_img_folder_path+path[2]+\"/\"+path[3], img_c)\n",
        "        for path in self.trainimgs:\n",
        "            image = cv2.imread(path[0] + path[1] + path[2] + \"/\" + path[3], cv2.IMREAD_GRAYSCALE)\n",
        "            img_c = cv2.Canny(image, 30, 80)\n",
        "            cv2.imwrite(path[0] + self.model2_img_folder_path + path[2]+\"/\" + path[3], img_c)\n",
        "        print(\"생성 완료\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "IMGP = Image_propressing()\n",
        "IMGP.reset_folder()  # 폴더 초기화\n",
        "IMGP.set_folder()\n",
        "IMGP.save_train()\n",
        "#IMGP.made_model1_dataset()\n",
        "IMGP.made_model2_dataset()\n",
        "\"\"\"\n",
        "IMGP.train_path = \"train1/\"\n",
        "IMGP.val_path = \"val1/\"\n",
        "IMGP.train_save = 4\n",
        "IMGP.val_save = 13\n",
        "IMGP.set_folder()\n",
        "IMGP.save_train()\n",
        "IMGP.made_model1_dataset()\n",
        "IMGP.made_model2_dataset()\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        },
        "id": "ZwvF52xsqCt9",
        "outputId": "6d345f7d-01dd-4347-ae44-c4185fdf026a"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "폴더 초기화\n",
            "setting folder\n",
            "error_00: _ari: 동영상 없음 \n",
            "error_00: _smallthe: 동영상 없음 \n",
            "생성 완료\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nIMGP.train_path = \"train1/\"\\nIMGP.val_path = \"val1/\"\\nIMGP.train_save = 4\\nIMGP.val_save = 13\\nIMGP.set_folder()\\nIMGP.save_train()\\nIMGP.made_model1_dataset()\\nIMGP.made_model2_dataset()\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "V1OI3MGjAxrQ"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import models, layers\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.ensemble import VotingClassifier,BaggingClassifier\n",
        "\n",
        "class Model:\n",
        "    def __init__(self):\n",
        "        self.models = []\n",
        "        self.model  = None\n",
        "        self.model1 = None\n",
        "        self.model2 = None\n",
        "        self.train_dir = \"train/nature\"\n",
        "        self.val_dir = \"val/nature\"\n",
        "        self.train_dir1 = \"train/model2\"\n",
        "        self.val_dir1 = \"val/model2\"\n",
        "        self.bagging_model1 = None\n",
        "        self.bagging_model2 = None\n",
        "        self.train_dataset = None\n",
        "        self.val_dataset = None\n",
        "        self.color =\"rgb\"\n",
        "        self.gray = \"gray\"\n",
        "        self.epoch = 10\n",
        "    def setmodel1(self):\n",
        "        self.model1 = models.Sequential()\n",
        "        self.model1.add( layers.Conv2D(filters = 32,kernel_size = (3,3), activation= 'relu', input_shape=(240,240,3), strides = (1,1), padding = 'same')) #128\n",
        "        self.model1.add( layers.BatchNormalization())\n",
        "        self.model1.add( layers.MaxPooling2D(2,2))\n",
        "        self.model1.add( layers.Conv2D(filters = 64,kernel_size =(3,3), activation='relu'))   #64\n",
        "        self.model1.add( layers.BatchNormalization())\n",
        "        self.model1.add( layers.MaxPooling2D(2,2))\n",
        "        self.model1.add( layers.Conv2D(filters = 32,  kernel_size =(5,5), activation= 'relu'))   #32\n",
        "        self.model1.add( layers.BatchNormalization())\n",
        "        self.model1.add( layers.MaxPooling2D(2,2))\n",
        "        self.model1.add( layers.Flatten())\n",
        "        self.model1.add( layers.Dense(units=512, activation= 'relu'))\n",
        "        self.model1.add( layers.Dropout(0.5))\n",
        "        self.model1.add(layers.Dense(units=10, activation='softmax'))\n",
        "        self.model1.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "        print(\"모델1 구조 설정\")\n",
        "        return self.model1\n",
        "\n",
        "    def setmodel2(self):\n",
        "        self.model2 = models.Sequential()\n",
        "\n",
        "        self.model2.add(\n",
        "            layers.Conv2D(filters=32, kernel_size=(3, 3), activation='relu', input_shape=(240, 240, 1),\n",
        "                          strides=(1, 1), padding='same'))  # 128\n",
        "        self.model2.add(layers.BatchNormalization())\n",
        "        self.model2.add(layers.MaxPooling2D(2, 2))\n",
        "        self.model2.add(layers.Conv2D(filters=64, kernel_size=(3, 3), activation='relu')) # 64\n",
        "        self.model2.add(layers.BatchNormalization())\n",
        "        self.model2.add(layers.MaxPooling2D(2, 2))\n",
        "        self.model2.add(layers.Conv2D(filters=32, kernel_size=(5, 5), activation='relu'))  # 32\n",
        "        self.model2.add(layers.BatchNormalization())\n",
        "        self.model2.add(layers.MaxPooling2D(2, 2))\n",
        "        self.model2.add(layers.Flatten())\n",
        "        self.model2.add(layers.Dense(units=512, activation='relu'))\n",
        "        self.model2.add(layers.Dropout(0.5))\n",
        "        self.model2.add(layers.Dense(units=10, activation='softmax'))\n",
        "        self.model2.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "        print(\"모델2 구조 설정\")\n",
        "        return self.model2\n",
        "    def made_data(self, train_dir, val_dir, color):\n",
        "        train_datagen = ImageDataGenerator(\n",
        "            rescale = 1./255, zoom_range = 0.3,rotation_range=180)# 학습시마다 변경된 이미지\n",
        "        val_datagen = ImageDataGenerator(rotation_range=180, rescale = 1./255)\n",
        "        if color == \"rgb\":\n",
        "            train_generator = train_datagen.flow_from_directory(\n",
        "                train_dir,\n",
        "                target_size=(240, 240),\n",
        "                batch_size=16,\n",
        "                class_mode = 'categorical',\n",
        "                color_mode='rgb')\n",
        "            val_generator = val_datagen.flow_from_directory(\n",
        "                val_dir,\n",
        "                target_size=(240, 240),\n",
        "                batch_size=16,\n",
        "                class_mode = 'categorical',color_mode='rgb') #binary\n",
        "            self.train_dataset = tf.data.Dataset.from_generator(\n",
        "            lambda: train_generator,\n",
        "            output_signature=(\n",
        "                tf.TensorSpec(shape=(None, 240, 240, 3), dtype=tf.float32),\n",
        "                tf.TensorSpec(shape=(None, 10), dtype=tf.float32)))\n",
        "\n",
        "            self.val_dataset = tf.data.Dataset.from_generator(\n",
        "                lambda: val_generator,\n",
        "                output_signature=(\n",
        "                    tf.TensorSpec(shape=(None, 240, 240, 3), dtype=tf.float32),\n",
        "                    tf.TensorSpec(shape=(None, 10), dtype=tf.float32)))\n",
        "        elif color == \"gray\":\n",
        "            train_generator = train_datagen.flow_from_directory(\n",
        "                train_dir,\n",
        "                target_size=(240, 240),\n",
        "                batch_size=16,\n",
        "                class_mode='categorical',\n",
        "                color_mode='grayscale')\n",
        "            val_generator = val_datagen.flow_from_directory(\n",
        "                val_dir,\n",
        "                target_size=(240, 240),\n",
        "                batch_size=16,\n",
        "                class_mode='categorical', color_mode='grayscale')  # binary\n",
        "\n",
        "            self.train_dataset1 = tf.data.Dataset.from_generator(\n",
        "            lambda: train_generator,\n",
        "            output_signature=(\n",
        "                tf.TensorSpec(shape=(None, 240, 240, 1), dtype=tf.float32),\n",
        "                tf.TensorSpec(shape=(None, 10), dtype=tf.float32)))\n",
        "\n",
        "            self.val_dataset1 = tf.data.Dataset.from_generator(\n",
        "                lambda: val_generator,\n",
        "                output_signature=(\n",
        "                    tf.TensorSpec(shape=(None, 240, 240, 1), dtype=tf.float32),\n",
        "                    tf.TensorSpec(shape=(None, 10), dtype=tf.float32)))\n",
        "            # tf.data.Dataset으로 변환\n",
        "        self.steps_per_epoch = train_generator.samples // train_generator.batch_size\n",
        "        self.validation_steps = val_generator.samples // val_generator.batch_size\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def history(self):\n",
        "        plt.plot(self.history_model1.history['accuracy'])\n",
        "        plt.plot(self.history_model1.history['val_accuracy'])\n",
        "        plt.plot(self.history_model2.history['accuracy'])\n",
        "        plt.plot(self.history_model2.history['val_accuracy'])\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.xlabel('Accuracy')\n",
        "        plt.legend(['Train_model1', 'Test_model1','Train_model2', 'Test_model2'], loc='upper left')\n",
        "        plt.show()\n",
        "\n",
        "    def set_models(self):\n",
        "        self.models.append(self.model1)\n",
        "        self.models.append(self.model2)\n",
        "        print(\"모델 저장\")\n",
        "    def save_model(self, model,filepath):\n",
        "        model.save(filepath)\n",
        "        print(filepath + \"모델 저장\")\n",
        "    def made_voting_model(self, model1, model2):\n",
        "        self.model = [model1, model2]\n",
        "    def made_model(self,model):\n",
        "        if model == \"natural\":\n",
        "            # 앙상블 모델 함수 활성화 추가 코드\n",
        "            self.model1 = self.setmodel1()\n",
        "\n",
        "            self.made_data(self.train_dir, self.val_dir, self.color) #model1\n",
        "            self.history_model1 = self.model1.fit(self.train_dataset, epochs=self.epoch,steps_per_epoch= self.steps_per_epoch, validation_data= self.val_dataset,validation_steps=self.validation_steps)\n",
        "            self.history()\n",
        "            return self.model1\n",
        "        elif model == \"canny\":\n",
        "            self.model2 = self.setmodel2()\n",
        "            self.made_data(self.train_dir1, self.val_dir1, self.gray) #model2\n",
        "            self.history_model2 = self.model2.fit( self.train_dataset1, epochs=self.epoch,steps_per_epoch= self.steps_per_epoch, validation_data= self.val_dataset1, validation_steps=self.validation_steps)\n",
        "            self.history()\n",
        "            return self.model2\n",
        "\n",
        "    def predict(self, model):\n",
        "        pred = model.predict()\n",
        "        return pred\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U5M77FruAxrQ",
        "outputId": "65a41a79-4df9-4a0f-f225-9dfc9d9f8de2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "모델2 구조 설정\n",
            "Found 4872 images belonging to 10 classes.\n",
            "Found 852 images belonging to 10 classes.\n",
            "Epoch 1/30\n",
            "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 104ms/step - accuracy: 0.2421 - loss: 7.9925 - val_accuracy: 0.2347 - val_loss: 149.8133\n",
            "Epoch 2/30\n",
            "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 105ms/step - accuracy: 0.3148 - loss: 1.8045 - val_accuracy: 0.2428 - val_loss: 25.6169\n",
            "Epoch 3/30\n",
            "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 105ms/step - accuracy: 0.3916 - loss: 1.5495 - val_accuracy: 0.2978 - val_loss: 6.8527\n",
            "Epoch 4/30\n",
            "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 102ms/step - accuracy: 0.4336 - loss: 1.4480 - val_accuracy: 0.2476 - val_loss: 23.5492\n",
            "Epoch 5/30\n",
            "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 104ms/step - accuracy: 0.4869 - loss: 1.2848 - val_accuracy: 0.3493 - val_loss: 5.4198\n",
            "Epoch 6/30\n",
            "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 103ms/step - accuracy: 0.5196 - loss: 1.2246 - val_accuracy: 0.4605 - val_loss: 2.6325\n",
            "Epoch 7/30\n",
            "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 100ms/step - accuracy: 0.5140 - loss: 1.2367 - val_accuracy: 0.5849 - val_loss: 1.3069\n",
            "Epoch 8/30\n",
            "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 98ms/step - accuracy: 0.5414 - loss: 1.1215 - val_accuracy: 0.6029 - val_loss: 1.0180\n",
            "Epoch 9/30\n",
            "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 101ms/step - accuracy: 0.5643 - loss: 1.0981 - val_accuracy: 0.2931 - val_loss: 2.1915\n",
            "Epoch 10/30\n",
            "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 126ms/step - accuracy: 0.5694 - loss: 1.0559 - val_accuracy: 0.5969 - val_loss: 0.9850\n",
            "Epoch 11/30\n",
            "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 122ms/step - accuracy: 0.5588 - loss: 1.1164 - val_accuracy: 0.5275 - val_loss: 1.8615\n",
            "Epoch 12/30\n",
            "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 99ms/step - accuracy: 0.6086 - loss: 0.9965 - val_accuracy: 0.7117 - val_loss: 0.7951\n",
            "Epoch 13/30\n",
            "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 127ms/step - accuracy: 0.6137 - loss: 0.9714 - val_accuracy: 0.7249 - val_loss: 0.7718\n",
            "Epoch 14/30\n",
            "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 98ms/step - accuracy: 0.6079 - loss: 0.9781 - val_accuracy: 0.6220 - val_loss: 1.1586\n",
            "Epoch 15/30\n",
            "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 100ms/step - accuracy: 0.6223 - loss: 0.9644 - val_accuracy: 0.6471 - val_loss: 0.8619\n",
            "Epoch 16/30\n",
            "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 106ms/step - accuracy: 0.6330 - loss: 0.9155 - val_accuracy: 0.6041 - val_loss: 1.8095\n",
            "Epoch 17/30\n",
            "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 105ms/step - accuracy: 0.6551 - loss: 0.8731 - val_accuracy: 0.4761 - val_loss: 2.3665\n",
            "Epoch 18/30\n",
            "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 99ms/step - accuracy: 0.6581 - loss: 0.8853 - val_accuracy: 0.7608 - val_loss: 0.5758\n",
            "Epoch 19/30\n",
            "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 100ms/step - accuracy: 0.6577 - loss: 0.8637 - val_accuracy: 0.5383 - val_loss: 1.1856\n",
            "Epoch 20/30\n",
            "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 106ms/step - accuracy: 0.6731 - loss: 0.8316 - val_accuracy: 0.7189 - val_loss: 0.8322\n",
            "Epoch 21/30\n",
            "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 100ms/step - accuracy: 0.6688 - loss: 0.8268 - val_accuracy: 0.7632 - val_loss: 0.7420\n",
            "Epoch 22/30\n",
            "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 99ms/step - accuracy: 0.6717 - loss: 0.8038 - val_accuracy: 0.8086 - val_loss: 0.4975\n",
            "Epoch 23/30\n",
            "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 97ms/step - accuracy: 0.6940 - loss: 0.7613 - val_accuracy: 0.7691 - val_loss: 0.6906\n",
            "Epoch 24/30\n",
            "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 102ms/step - accuracy: 0.6837 - loss: 0.7587 - val_accuracy: 0.8086 - val_loss: 0.5340\n",
            "Epoch 25/30\n",
            "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 100ms/step - accuracy: 0.6934 - loss: 0.7352 - val_accuracy: 0.7799 - val_loss: 0.5489\n",
            "Epoch 26/30\n",
            "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 98ms/step - accuracy: 0.7155 - loss: 0.7345 - val_accuracy: 0.7967 - val_loss: 0.6908\n",
            "Epoch 27/30\n",
            "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 98ms/step - accuracy: 0.7065 - loss: 0.7280 - val_accuracy: 0.7883 - val_loss: 0.4972\n",
            "Epoch 28/30\n",
            "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 100ms/step - accuracy: 0.7088 - loss: 0.7037 - val_accuracy: 0.8110 - val_loss: 0.5118\n",
            "Epoch 29/30\n",
            "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 104ms/step - accuracy: 0.7480 - loss: 0.6609 - val_accuracy: 0.5048 - val_loss: 9.0285\n",
            "Epoch 30/30\n",
            "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 99ms/step - accuracy: 0.7046 - loss: 0.7547 - val_accuracy: 0.7823 - val_loss: 0.8982\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "canny.h5모델 저장\n",
            "모델1 구조 설정\n",
            "Found 4872 images belonging to 10 classes.\n",
            "Found 852 images belonging to 10 classes.\n",
            "Epoch 1/30\n",
            "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m89s\u001b[0m 277ms/step - accuracy: 0.3520 - loss: 6.7725 - val_accuracy: 0.0837 - val_loss: 8.2514\n",
            "Epoch 2/30\n",
            "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m88s\u001b[0m 279ms/step - accuracy: 0.5485 - loss: 1.5087 - val_accuracy: 0.5084 - val_loss: 1.9534\n",
            "Epoch 3/30\n",
            "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 276ms/step - accuracy: 0.6271 - loss: 1.0618 - val_accuracy: 0.7572 - val_loss: 0.8071\n",
            "Epoch 4/30\n",
            "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 276ms/step - accuracy: 0.6944 - loss: 0.9178 - val_accuracy: 0.6615 - val_loss: 0.9992\n",
            "Epoch 5/30\n",
            "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 272ms/step - accuracy: 0.7258 - loss: 0.8098 - val_accuracy: 0.8792 - val_loss: 0.3341\n",
            "Epoch 6/30\n",
            "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 269ms/step - accuracy: 0.7648 - loss: 0.6834 - val_accuracy: 0.7488 - val_loss: 0.7354\n",
            "Epoch 7/30\n",
            "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 271ms/step - accuracy: 0.7517 - loss: 0.7590 - val_accuracy: 0.8792 - val_loss: 0.3827\n",
            "Epoch 8/30\n",
            "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 270ms/step - accuracy: 0.8194 - loss: 0.5701 - val_accuracy: 0.8684 - val_loss: 0.4116\n",
            "Epoch 9/30\n",
            "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 271ms/step - accuracy: 0.8452 - loss: 0.4781 - val_accuracy: 0.9091 - val_loss: 0.2234\n",
            "Epoch 10/30\n",
            "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 268ms/step - accuracy: 0.8483 - loss: 0.4803 - val_accuracy: 0.8971 - val_loss: 0.3240\n",
            "Epoch 11/30\n",
            "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 269ms/step - accuracy: 0.8506 - loss: 0.4581 - val_accuracy: 0.9103 - val_loss: 0.2769\n",
            "Epoch 12/30\n",
            "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 269ms/step - accuracy: 0.8634 - loss: 0.4121 - val_accuracy: 0.6926 - val_loss: 2.7496\n",
            "Epoch 13/30\n",
            "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 270ms/step - accuracy: 0.8789 - loss: 0.3721 - val_accuracy: 0.9007 - val_loss: 0.4065\n",
            "Epoch 14/30\n",
            "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 269ms/step - accuracy: 0.8805 - loss: 0.3852 - val_accuracy: 0.8457 - val_loss: 0.5218\n",
            "Epoch 15/30\n",
            "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 269ms/step - accuracy: 0.8817 - loss: 0.3942 - val_accuracy: 0.9378 - val_loss: 0.1589\n",
            "Epoch 16/30\n",
            "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 270ms/step - accuracy: 0.9123 - loss: 0.2489 - val_accuracy: 0.9330 - val_loss: 0.2153\n",
            "Epoch 17/30\n",
            "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 270ms/step - accuracy: 0.8977 - loss: 0.3308 - val_accuracy: 0.8947 - val_loss: 0.4100\n",
            "Epoch 18/30\n",
            "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 270ms/step - accuracy: 0.8954 - loss: 0.3830 - val_accuracy: 0.9306 - val_loss: 0.2788\n",
            "Epoch 19/30\n",
            "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 270ms/step - accuracy: 0.9175 - loss: 0.2818 - val_accuracy: 0.7033 - val_loss: 1.4588\n",
            "Epoch 20/30\n",
            "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 269ms/step - accuracy: 0.9165 - loss: 0.3120 - val_accuracy: 0.9486 - val_loss: 0.2307\n",
            "Epoch 21/30\n",
            "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 267ms/step - accuracy: 0.9226 - loss: 0.3036 - val_accuracy: 0.9557 - val_loss: 0.1528\n",
            "Epoch 22/30\n",
            "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 270ms/step - accuracy: 0.9258 - loss: 0.2298 - val_accuracy: 0.9306 - val_loss: 0.2299\n",
            "Epoch 23/30\n",
            "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 272ms/step - accuracy: 0.9263 - loss: 0.2613 - val_accuracy: 0.9593 - val_loss: 0.1485\n",
            "Epoch 24/30\n",
            "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m89s\u001b[0m 292ms/step - accuracy: 0.9280 - loss: 0.2350 - val_accuracy: 0.9294 - val_loss: 0.3048\n",
            "Epoch 25/30\n",
            "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m87s\u001b[0m 286ms/step - accuracy: 0.9475 - loss: 0.1603 - val_accuracy: 0.9522 - val_loss: 0.1976\n",
            "Epoch 26/30\n",
            "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m97s\u001b[0m 319ms/step - accuracy: 0.9474 - loss: 0.1901 - val_accuracy: 0.9569 - val_loss: 0.1580\n",
            "Epoch 27/30\n",
            "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m99s\u001b[0m 328ms/step - accuracy: 0.9489 - loss: 0.1792 - val_accuracy: 0.8385 - val_loss: 0.9544\n",
            "Epoch 28/30\n",
            "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m97s\u001b[0m 321ms/step - accuracy: 0.9295 - loss: 0.2866 - val_accuracy: 0.7739 - val_loss: 2.1162\n",
            "Epoch 29/30\n",
            "\u001b[1m299/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m1s\u001b[0m 270ms/step - accuracy: 0.9306 - loss: 0.2813"
          ]
        }
      ],
      "source": [
        "model = Model()\n",
        "model.epoch = 30\n",
        "model2 = model.made_model(\"canny\")\n",
        "model.save_model(model2, \"canny.h5\")\n",
        "model1 = model.made_model(\"natural\")\n",
        "model.save_model(model1, \"model.h5\")\n",
        "\n",
        "# 배깅\n",
        "\"\"\"\n",
        "model.train_dir = \"train1/model1\"\n",
        "model.val_dir = \"val1/model1\"\n",
        "model.train_dir1 = \"train1/model2\"\n",
        "model.val_dir1 = \"val1/model2\"\n",
        "model3 = model.made_model(\"canny\")\n",
        "model4 = model.made_model(\"natural\")\n",
        "voting_model2 = model.made_voting_model()\n",
        "\"\"\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_canny_images(img):\n",
        "    processed_images = []\n",
        "    gray_img = cv2.cvtColor((img * 255).astype(np.uint8), cv2.COLOR_RGB2GRAY)\n",
        "    edges = cv2.Canny(gray_img, 50, 80)\n",
        "    return edges\n",
        "\n",
        "def voting_model_predict(model1,model2,img):\n",
        "   edges = preprocess_canny_images(img)\n",
        "   pred1 = model1.predict(img)\n",
        "   pred2 = model2.predict(edges)\n",
        "   predict = (pred1 * 0.8 + pred2 * 0.2)\n",
        "   return predict\n",
        "for img in model.val_dataset:\n",
        "  predict = voting_model_predict(model1,model2,img )\n"
      ],
      "metadata": {
        "id": "a5Qde57xGuqY"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
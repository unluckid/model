{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jX7ukQPOAxrM"
      },
      "source": [
        "<table align=\"left\">\n",
        "  <td>\n",
        "    <a href=\"https://colab.research.google.com/github/rickiepark/handson-ml3/blob/main/12_custom_models_and_training_with_tensorflow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sFcMpxP2AxrN",
        "tags": []
      },
      "source": [
        "# 설정"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "xNKQVwanAxrN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 148
        },
        "outputId": "87e7bd4c-90ae-4d73-a7fc-f4ad34e1d540"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=181xBl-rU9gBAL8-iEZbPNxKzPZY-QCGu\n",
            "From (redirected): https://drive.google.com/uc?id=181xBl-rU9gBAL8-iEZbPNxKzPZY-QCGu&confirm=t&uuid=256ef4cc-b176-4be4-a8bb-e3937dd4052d\n",
            "To: /content/dataset.zip\n",
            "100%|██████████| 1.65G/1.65G [00:29<00:00, 56.6MB/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/dataset.zip'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "import gdown\n",
        "import numpy as np\n",
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "import shutil\n",
        "file_id = '181xBl-rU9gBAL8-iEZbPNxKzPZY-QCGu'\n",
        "download_url = f'https://drive.google.com/uc?id={file_id}'\n",
        "\n",
        "\n",
        "destination_path = '/content/dataset.zip'  # 파일 경로 설정\n",
        "\n",
        "# 다운로드 시작\n",
        "gdown.download(download_url, destination_path, quiet=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "0Ddo9OPsAxrO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c810612d-a6fb-4df4-8f54-3caa4eadc738"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "압축이 /content/에 해제되었습니다.\n"
          ]
        }
      ],
      "source": [
        "import zipfile\n",
        "\n",
        "\n",
        "zip_file_path = '/content/dataset.zip'\n",
        "extract_path = '/content/'\n",
        "\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n",
        "\n",
        "print(f\"압축이 {extract_path}에 해제되었습니다.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.makedirs(\"train/\", exist_ok=True)\n",
        "os.makedirs(\"val/\", exist_ok=True)"
      ],
      "metadata": {
        "id": "cvsDrFm3i-MP"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "JN8_1_vCAxrP"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "class Image_propressing:\n",
        "    def __init__(self):\n",
        "        #  폴더 경로\n",
        "        self.video_path = \"video/\"  # 비디오 폴더\n",
        "        self.nature_img_folder_path = \"nature/\"  # 전처리 이전 폴더\n",
        "        self.model1_img_folder_path = \"model1/\"  # 기본 모델 폴더\n",
        "        self.model2_img_folder_path = \"model2/\"  # 캐니 모델 폴더\n",
        "        self.train_path = \"train/\"\n",
        "        self.val_path = \"val/\"\n",
        "        self.data = []  # 전처리 전 데이터셋\n",
        "\n",
        "        self.label = [\"_ari\", \"_smallthe\", \"_corki\", \"_velkoz\", \"_thresh\", \"_gragas\", \"_atrox\", \"_kogmaw\", \"_8\", \"_9\"]  # 10개의 이미지 폴더에 대한 값\n",
        "        self.model_data1 = []  # 기본 이미지 모델 데이터셋\n",
        "        self.model_data2 = []  # 캐니 이미지 모델 데이터셋\n",
        "        self.valimgs = []    # 폴더 내 이미지 파일 이름을 저장할 리스트\n",
        "        self.trainimgs = []  # 폴더 내 이미지 파일 이름을 저장할 리스트\n",
        "        self.train_save = 3\n",
        "        self.val_save = 11\n",
        "    def reset_folder(self):\n",
        "        shutil.rmtree(self.train_path)\n",
        "        shutil.rmtree(self.val_path)\n",
        "        print(\"폴더 초기화\")\n",
        "    def set_folder(self):\n",
        "        os.makedirs(self.video_path, exist_ok=True)\n",
        "        os.makedirs(self.train_path + self.nature_img_folder_path, exist_ok=True)\n",
        "        os.makedirs(self.train_path + self.model1_img_folder_path, exist_ok=True)\n",
        "        os.makedirs(self.train_path + self.model2_img_folder_path, exist_ok=True)\n",
        "        os.makedirs(self.val_path + self.nature_img_folder_path, exist_ok=True)\n",
        "        os.makedirs(self.val_path + self.model1_img_folder_path, exist_ok=True)\n",
        "        os.makedirs(self.val_path + self.model2_img_folder_path, exist_ok=True)\n",
        "        for label in self.label:                   # 10개의 하위 폴더 생성 코드 추가 작성\n",
        "            os.makedirs(self.video_path + label, exist_ok=True)\n",
        "            os.makedirs(self.train_path + self.nature_img_folder_path + label, exist_ok=True)\n",
        "            os.makedirs(self.train_path + self.model1_img_folder_path + label, exist_ok=True)\n",
        "            os.makedirs(self.train_path + self.model2_img_folder_path + label, exist_ok=True)\n",
        "            os.makedirs(self.val_path + self.nature_img_folder_path + label, exist_ok=True)\n",
        "            os.makedirs(self.val_path + self.model1_img_folder_path + label, exist_ok=True)\n",
        "            os.makedirs(self.val_path + self.model2_img_folder_path + label, exist_ok=True)\n",
        "        print(\"setting folder\")\n",
        "\n",
        "    def save_train(self):\n",
        "        for label in self.label:\n",
        "            for file_name in os.listdir(self.video_path + label):\n",
        "                file_path = os.path.join(self.video_path + label, file_name)\n",
        "                video_capture = cv2.VideoCapture(file_path) # 동영상 로드\n",
        "                if not video_capture.isOpened():\n",
        "                    print(\"error_00: \" + label + \": 동영상 없음 \")\n",
        "                frame_count, train_count, val_count = 0, 0, 0\n",
        "                fps = video_capture.get(cv2.CAP_PROP_FPS)  # fps 설정\n",
        "                train_save = int(fps * self.train_save/10)  # 훈련에 사용될 이미지 정리  3초 간격으로 작성\n",
        "                val_save = int(fps * self.val_save/10)   # 훈련에 사용될 이미지 정리  11초 간격으로 작성\n",
        "                while True:\n",
        "                    ret, frame = video_capture.read()\n",
        "                    if not ret:\n",
        "                        break  # 프레임이 더 이상 없으면 반복문 종료\n",
        "                    # 프레임 파일 이름 형식 지정 및 저장\n",
        "                    if frame_count % train_save == 0:\n",
        "                        frame_filename = os.path.join(self.train_path + self.nature_img_folder_path + label, f\"{file_name}_{train_count:04d}.png\")\n",
        "                        frame = cv2.resize(frame, (240, 240))  # 이미지 크기 미리 정리\n",
        "                        cv2.imwrite(frame_filename, frame)\n",
        "                        train_count += 1\n",
        "                    elif frame_count % val_save == 0:  # elif 이미지 중복 제거\n",
        "                        frame_filename = os.path.join(self.val_path + self.nature_img_folder_path + label, f\"{file_name}_{val_count:04d}.png\")\n",
        "                        frame = cv2.resize(frame, (240, 240))  # 이미지 크기 미리 정리\n",
        "                        cv2.imwrite(frame_filename, frame)\n",
        "                        val_count += 1\n",
        "                    frame_count += 1\n",
        "                    # 자원 해제\n",
        "                video_capture.release()\n",
        "\n",
        "    # 이미지의 폴더 패스를 받고 이를 self.imgs 에 저장한다.\n",
        "    def set_img(self, model): #이미지 이름을 모두 따서 self.imgs에 저장해둠\n",
        "        self.valimgs = []\n",
        "        self.trainimgs = []\n",
        "        for label in self.label:\n",
        "            for img in os.listdir(self.val_path + model + label):\n",
        "                if os.path.splitext(img)[1].lower() in {\".png\", \".jpg\"}:\n",
        "                    self.valimgs.append([self.val_path, model, label, img])\n",
        "                else:\n",
        "                    print(\"error_01: 이미지 전처리 에러\")\n",
        "        for label in self.label:\n",
        "            for img in os.listdir(self.train_path + model + label):\n",
        "                if os.path.splitext(img)[1].lower() in {\".png\", \".jpg\"}:\n",
        "                    self.trainimgs.append([self.train_path, model, label, img])\n",
        "                else:\n",
        "                    print(\"error_01: 이미지 전처리 에러\")\n",
        "    \"\"\"\n",
        "    # 이미지 로테이트 함수\n",
        "    def made_model1_dataset(self):\n",
        "        self.set_img(self.nature_img_folder_path)\n",
        "        for path in self.valimgs:\n",
        "            image = cv2.imread(path[0]+path[1]+path[2]+\"/\"+path[3], cv2.IMREAD_COLOR)\n",
        "            height, width = image.shape[:2]\n",
        "            for i in range(12):\n",
        "                r_matrix = cv2.getRotationMatrix2D((width // 2, height // 2), i * 30, 1.0)\n",
        "                r_img = cv2.warpAffine(image, r_matrix, (width, height))\n",
        "                cv2.imwrite(self.val_path+self.model1_img_folder_path+path[2]+\"/\"+str(i)+path[3], r_img)\n",
        "        for path in self.trainimgs:\n",
        "            image = cv2.imread(path[0]+path[1]+path[2]+\"/\"+path[3], cv2.IMREAD_COLOR)\n",
        "            height, width = image.shape[:2]\n",
        "            for i in range(12):\n",
        "                r_matrix = cv2.getRotationMatrix2D((width // 2, height // 2), i * 30, 1.0)\n",
        "                r_img = cv2.warpAffine(image, r_matrix, (width, height))\n",
        "                cv2.imwrite(self.train_path+self.model1_img_folder_path+path[2]+\"/\"+str(i)+path[3], r_img)\n",
        "        # return self.model_data1    # data리스트 반환\n",
        "      \"\"\"\n",
        "    def made_model2_dataset(self):\n",
        "        self.set_img(self.nature_img_folder_path)\n",
        "        for path in self.valimgs:\n",
        "            image = cv2.imread(path[0] + path[1] + path[2]+\"/\"+path[3], cv2.IMREAD_GRAYSCALE)\n",
        "            img_c = cv2.Canny(image, 30, 80)\n",
        "            cv2.imwrite(path[0]+self.model2_img_folder_path+path[2]+\"/\"+path[3], img_c)\n",
        "        for path in self.trainimgs:\n",
        "            image = cv2.imread(path[0] + path[1] + path[2] + \"/\" + path[3], cv2.IMREAD_GRAYSCALE)\n",
        "            img_c = cv2.Canny(image, 30, 80)\n",
        "            cv2.imwrite(path[0] + self.model2_img_folder_path + path[2]+\"/\" + path[3], img_c)\n",
        "        print(\"생성 완료\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "IMGP = Image_propressing()\n",
        "IMGP.reset_folder()  # 폴더 초기화\n",
        "IMGP.set_folder()\n",
        "IMGP.save_train()\n",
        "#IMGP.made_model1_dataset() # 로테이트 이미지 생성자 사용 안함\n",
        "#IMGP.made_model2_dataset() # canny이미지 생성자 사용 안함\n",
        "\"\"\"\n",
        "IMGP.train_path = \"train1/\"\n",
        "IMGP.val_path = \"val1/\"\n",
        "IMGP.train_save = 4\n",
        "IMGP.val_save = 13\n",
        "IMGP.set_folder()\n",
        "IMGP.save_train()\n",
        "IMGP.made_model1_dataset()\n",
        "IMGP.made_model2_dataset()\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 146
        },
        "id": "ZwvF52xsqCt9",
        "outputId": "9bb33188-f5e3-4ff6-c453-55decfe4f579"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "폴더 초기화\n",
            "setting folder\n",
            "error_00: _ari: 동영상 없음 \n",
            "error_00: _smallthe: 동영상 없음 \n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nIMGP.train_path = \"train1/\"\\nIMGP.val_path = \"val1/\"\\nIMGP.train_save = 4\\nIMGP.val_save = 13\\nIMGP.set_folder()\\nIMGP.save_train()\\nIMGP.made_model1_dataset()\\nIMGP.made_model2_dataset()\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "V1OI3MGjAxrQ"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import models, layers\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.ensemble import VotingClassifier,BaggingClassifier\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 369
        },
        "id": "U5M77FruAxrQ",
        "outputId": "3f1dc7a6-de0e-45ab-85ac-93ead14b1b79"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'Model' object has no attribute 'setmodel1'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-d6e2262268dd>\u001b[0m in \u001b[0;36m<cell line: 110>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m \u001b[0mmodel1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmade_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"natural\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"model.h5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-d6e2262268dd>\u001b[0m in \u001b[0;36mmade_model\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"natural\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m             \u001b[0;31m# 앙상블 모델 함수 활성화 추가 코드\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetmodel1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmade_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mval_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#model1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'Model' object has no attribute 'setmodel1'"
          ]
        }
      ],
      "source": [
        "# 모델 학습 시 캐니 이미지의 정확성이 더 적게 나오는 것을 확인하여  rgb이미지로 변경하여 학습하였음\n",
        "class Model:\n",
        "    def __init__(self):\n",
        "        self.models = []\n",
        "        self.model  = None\n",
        "        self.model1 = None\n",
        "        self.model2 = None\n",
        "        self.train_dir = \"train/nature\"\n",
        "        self.val_dir = \"val/nature\"\n",
        "        self.train_dir1 = \"train/model2\"\n",
        "        self.val_dir1 = \"val/model2\"\n",
        "        self.bagging_model1 = None\n",
        "        self.bagging_model2 = None\n",
        "        self.train_dataset = None\n",
        "        self.val_dataset = None\n",
        "        self.color =\"rgb\"\n",
        "        self.gray = \"gray\"\n",
        "        self.epoch = 10\n",
        "\n",
        "    def made_data(self, train_dir, val_dir, color):\n",
        "        train_datagen = ImageDataGenerator(\n",
        "            rescale = 1./255, zoom_range = 0.3,rotation_range=180)# 학습시마다 변경된 이미지\n",
        "        val_datagen = ImageDataGenerator(rotation_range=180, rescale = 1./255)\n",
        "        if color == \"rgb\":\n",
        "            train_generator = train_datagen.flow_from_directory(\n",
        "                train_dir,\n",
        "                target_size=(224, 224),\n",
        "                batch_size=16,\n",
        "                class_mode = 'categorical',\n",
        "                color_mode='rgb')\n",
        "            val_generator = val_datagen.flow_from_directory(\n",
        "                val_dir,\n",
        "                target_size=(224, 224),\n",
        "                batch_size=16,\n",
        "                class_mode = 'categorical',color_mode='rgb') #binary\n",
        "            self.train_dataset = tf.data.Dataset.from_generator(\n",
        "            lambda: train_generator,\n",
        "            output_signature=(\n",
        "                tf.TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32),\n",
        "                tf.TensorSpec(shape=(None, 10), dtype=tf.float32)))\n",
        "\n",
        "            self.val_dataset = tf.data.Dataset.from_generator(\n",
        "                lambda: val_generator,\n",
        "                output_signature=(\n",
        "                    tf.TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32),\n",
        "                    tf.TensorSpec(shape=(None, 10), dtype=tf.float32)))\n",
        "        elif color == \"gray\":\n",
        "            train_generator = train_datagen.flow_from_directory(\n",
        "                train_dir,\n",
        "                target_size=(224, 224),\n",
        "                batch_size=16,\n",
        "                class_mode='categorical',\n",
        "                color_mode='grayscale')\n",
        "            val_generator = val_datagen.flow_from_directory(\n",
        "                val_dir,\n",
        "                target_size=(224, 224),\n",
        "                batch_size=16,\n",
        "                class_mode='categorical', color_mode='grayscale')  # binary\n",
        "\n",
        "            self.train_dataset1 = tf.data.Dataset.from_generator(\n",
        "            lambda: train_generator,\n",
        "            output_signature=(\n",
        "                tf.TensorSpec(shape=(None, 224, 224, 1), dtype=tf.float32),\n",
        "                tf.TensorSpec(shape=(None, 10), dtype=tf.float32)))\n",
        "\n",
        "            self.val_dataset1 = tf.data.Dataset.from_generator(\n",
        "                lambda: val_generator,\n",
        "                output_signature=(\n",
        "                    tf.TensorSpec(shape=(None, 224, 224, 1), dtype=tf.float32),\n",
        "                    tf.TensorSpec(shape=(None, 10), dtype=tf.float32)))\n",
        "            # tf.data.Dataset으로 변환\n",
        "        self.steps_per_epoch = train_generator.samples // train_generator.batch_size\n",
        "        self.validation_steps = val_generator.samples // val_generator.batch_size\n",
        "\n",
        "    def history(self):\n",
        "        plt.plot(self.history_model1.history['accuracy'])\n",
        "        plt.plot(self.history_model1.history['val_accuracy'])\n",
        "        plt.plot(self.history_model2.history['accuracy'])\n",
        "        plt.plot(self.history_model2.history['val_accuracy'])\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.xlabel('Accuracy')\n",
        "        plt.legend(['Train_model1', 'Test_model1','Train_model2', 'Test_model2'], loc='upper left')\n",
        "        plt.show()\n",
        "    def save_model(self, model,filepath):\n",
        "        model.save(filepath)\n",
        "        print(filepath + \"모델 저장\")\n",
        "    def made_voting_model(self, model1, model2):\n",
        "        self.model = [model1, model2]\n",
        "    def made_model(self,model):\n",
        "        if model == \"natural\":\n",
        "            # 앙상블 모델 함수 활성화 추가 코드\n",
        "            self.model1 = self.setmodel1()\n",
        "\n",
        "            self.made_data(self.train_dir, self.val_dir, self.color) #model1\n",
        "            self.history_model1 = self.model1.fit(self.train_dataset, epochs=self.epoch,steps_per_epoch= self.steps_per_epoch, validation_data= self.val_dataset,validation_steps=self.validation_steps)\n",
        "            self.history()\n",
        "            return self.model1\n",
        "        elif model == \"canny\":\n",
        "            self.model2 = self.setmodel2()\n",
        "            self.made_data(self.train_dir1, self.val_dir1, self.gray) #model2\n",
        "            self.history_model2 = self.model2.fit( self.train_dataset1, epochs=self.epoch,steps_per_epoch= self.steps_per_epoch, validation_data= self.val_dataset1, validation_steps=self.validation_steps)\n",
        "            self.history()\n",
        "            return self.model2\n",
        "\n",
        "    def predict(self, model):\n",
        "        pred = model.predict()\n",
        "        return pred\n",
        "model = Model()\n",
        "model.epoch = 30\n",
        "model1 = model.made_model(\"natural\")\n",
        "model.save_model(model1, \"model.h5\")\n",
        "\n",
        "# 배깅\n",
        "\"\"\"\n",
        "model.train_dir = \"train1/model1\"\n",
        "model.val_dir = \"val1/model1\"\n",
        "model.train_dir1 = \"train1/model2\"\n",
        "model.val_dir1 = \"val1/model2\"\n",
        "model3 = model.made_model(\"canny\")\n",
        "model4 = model.made_model(\"natural\")\n",
        "voting_model2 = model.made_voting_model()\n",
        "\"\"\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.applications import VGG16, Xception ,ResNet50, InceptionV3\n",
        "train_dir = \"train/nature\"\n",
        "val_dir = \"val/nature\"\n",
        "# 데이터 증강 (학습 데이터에만 적용)\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=180,       # 회전 범위 증가\n",
        ")\n",
        "\n",
        "# 검증 데이터는 증강하지 않음 (단순히 정규화만 적용)\n",
        "val_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "# 학습 데이터 생성기\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_dir,  # 학습 데이터 경로\n",
        "    target_size=(224, 224),  # VGG16의 입력 크기에 맞게 리사이즈\n",
        "    batch_size=32,\n",
        "    class_mode='categorical',  # 다중 클래스 분류\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "# 검증 데이터 생성기\n",
        "val_generator = val_datagen.flow_from_directory(\n",
        "    val_dir,  # 검증 데이터 경로\n",
        "    target_size=(224, 224),  # VGG16의 입력 크기에 맞게 리사이즈\n",
        "    batch_size=32,\n",
        "    class_mode='categorical'  # 다중 클래스 분류\n",
        ")\n",
        "\n",
        "# tf.data.Dataset으로 변환\n",
        "train_dataset = tf.data.Dataset.from_generator(\n",
        "    lambda: train_generator,\n",
        "    output_signature=(\n",
        "        tf.TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32),\n",
        "        tf.TensorSpec(shape=(None, 10), dtype=tf.float32)\n",
        "    )\n",
        ")\n",
        "\n",
        "val_dataset = tf.data.Dataset.from_generator(\n",
        "    lambda: val_generator,\n",
        "    output_signature=(\n",
        "        tf.TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32),\n",
        "        tf.TensorSpec(shape=(None, 10), dtype=tf.float32)\n",
        "    )\n",
        ")\n",
        "train_dataset = train_dataset.repeat()  # 학습 데이터 무한 반복\n",
        "val_dataset = val_dataset.repeat()      # 검증 데이터 무한 반복\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TDznmrcxaSe1",
        "outputId": "b8231b57-fbaa-44de-b86a-5f7d9205e04e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 4872 images belonging to 10 classes.\n",
            "Found 852 images belonging to 10 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 보팅에 사용할 모델 불러오기 (pre-trained weights 사용, top layers 제외)\n",
        "vgg16_base = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "inceptionv3_base = InceptionV3(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "xception_base = Xception(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7SiBHS5MiHx9",
        "outputId": "c19b0c69-a1a1-4ca1-8ff5-078d3c618739"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "\u001b[1m58889256/58889256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "\u001b[1m94765736/94765736\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_v3/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "\u001b[1m87910968/87910968\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/xception/xception_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "\u001b[1m83683744/83683744\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def addlayer(model):\n",
        "# 모델에 Flatten 레이어 추가\n",
        "  x = layers.Flatten()(model.output)\n",
        "  # Dense 레이어 추가 (출력 클래스 수에 맞게)\n",
        "  x = layers.Dense(512, activation='relu')(x)\n",
        "  x = layers.Dropout(0.6)(x)  # Dropout 추가\n",
        "  x = layers.Dense(10, activation='softmax')(x)\n",
        "  made_model = models.Model(inputs=model.input, outputs=x)\n",
        "  return made_model"
      ],
      "metadata": {
        "id": "sXS_geIIhcQd"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vgg16 = addlayer(vgg16_base)\n",
        "inceptionv3 = addlayer(inceptionv3_base)\n",
        "xception = addlayer(xception_base)\n",
        "\n",
        "for layer in vgg16_base.layers:\n",
        "    layer.trainable = False\n",
        "for layer in inceptionv3_base.layers:\n",
        "    layer.trainable = False\n",
        "for layer in xception_base.layers:\n",
        "    layer.trainable = False\n",
        "# 모델 컴파일\n",
        "vgg16.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "inceptionv3.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "xception.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "# 모델 요약 정보 출력\n",
        "#vgg16.summary()\n",
        "#ResNet50.summary()\n",
        "#inceptionv3.summary()\n",
        "#xception.summary()"
      ],
      "metadata": {
        "id": "_yW-VKqGhehL"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# steps_per_epoch, validation_steps 설정\n",
        "steps_per_epoch = train_generator.samples // train_generator.batch_size\n",
        "validation_steps = val_generator.samples // val_generator.batch_size\n",
        "# 모델 학습\n",
        "def hist(model,train_dataset,steps_per_epoch,val_dataset,validation_steps):\n",
        "  history = model.fit(\n",
        "      train_dataset,  # tf.data.Dataset 사용\n",
        "      steps_per_epoch=steps_per_epoch,  # 배치 크기에 맞춰 학습 스텝 설정\n",
        "      epochs=50,  # 에포크 수\n",
        "      validation_data=val_dataset,  # 검증 데이터\n",
        "      validation_steps=validation_steps  # 검증 배치 크기 맞추기\n",
        "  )\n",
        "  return history\n",
        "#vgg16_h =  hist(vgg16,train_dataset,steps_per_epoch,val_dataset,validation_steps)\n",
        "inceptionv3 =  hist(vgg16,train_dataset,steps_per_epoch,val_dataset,validation_steps)\n",
        "inceptionv3.save('inceptionv3.keras')\n",
        "xception_h =  hist(vgg16,train_dataset,steps_per_epoch,val_dataset,validation_steps)\n",
        "xception.save('xception.keras')\n",
        "# 모델 저장\n",
        "#vgg16.save('vgg16.keras')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MSsRx7bPdn22",
        "outputId": "7d629c15-6f28-44a3-cd23-8d46377385b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3283s\u001b[0m 22s/step - accuracy: 0.4416 - loss: 2.0022 - val_accuracy: 0.7392 - val_loss: 0.8275\n",
            "Epoch 2/20\n",
            "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3188s\u001b[0m 21s/step - accuracy: 0.6282 - loss: 0.8885 - val_accuracy: 0.8183 - val_loss: 0.5758\n",
            "Epoch 3/20\n",
            "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3210s\u001b[0m 21s/step - accuracy: 0.6981 - loss: 0.7171 - val_accuracy: 0.8598 - val_loss: 0.5255\n",
            "Epoch 4/20\n",
            "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3172s\u001b[0m 21s/step - accuracy: 0.7135 - loss: 0.6829 - val_accuracy: 0.8634 - val_loss: 0.4298\n",
            "Epoch 5/20\n",
            "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3177s\u001b[0m 21s/step - accuracy: 0.7487 - loss: 0.6093 - val_accuracy: 0.8829 - val_loss: 0.3687\n",
            "Epoch 6/20\n",
            "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3176s\u001b[0m 21s/step - accuracy: 0.7585 - loss: 0.5762 - val_accuracy: 0.8854 - val_loss: 0.3352\n",
            "Epoch 7/20\n",
            "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3173s\u001b[0m 21s/step - accuracy: 0.7732 - loss: 0.5490 - val_accuracy: 0.9256 - val_loss: 0.3005\n",
            "Epoch 8/20\n",
            "\u001b[1m 61/152\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m27:19\u001b[0m 18s/step - accuracy: 0.7960 - loss: 0.5024"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "history_list = [vgg16_h,  inceptionv3_h, xception_h]\n",
        "model_names = ['VGG16',  'InceptionV3', 'Xception']\n",
        "\n",
        "# 각 모델에 대해 학습 및 검증 정확도와 손실을 그리는 반복문\n",
        "for i, history in enumerate(history_list):\n",
        "    plt.figure(figsize=(12, 5))\n",
        "\n",
        "    # 정확도 그래프\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(history.history['accuracy'])\n",
        "    plt.plot(history.history['val_accuracy'])\n",
        "    plt.title(f'{model_names[i]} - Accuracy')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "\n",
        "    # 손실 그래프\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(history.history['loss'])\n",
        "    plt.plot(history.history['val_loss'])\n",
        "    plt.title(f'{model_names[i]} - Loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "I6Uerrv6n_wA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import VotingClassifier\n",
        "named_estimators = [\n",
        "    (\"inceptionv3\", inceptionv3),\n",
        "    (\"xception_h\",  xception_h),\n",
        "    (\"vgg16\", vgg16),\n",
        "]\n",
        "voting_clf = VotingClassifier(named_estimators)"
      ],
      "metadata": {
        "id": "mF_iWbAEdQVl"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}